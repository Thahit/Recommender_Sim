{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "# Add the parent directory to the sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "#import simtrain\n",
    "from simtrain.sim_models_new import User_simmulation_Model\n",
    "from simtrain import SETTINGS_POLIMI as SETTINGS, process_dat, Dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import ast\n",
    "\n",
    "import paths\n",
    "from os.path import join\n",
    "import pytorch_warmup as warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items = 7\n",
    "num_items_per_recom = 2\n",
    "num_interaction_types = 2\n",
    "recom_dim = 1\n",
    "num_users = 11\n",
    "min_inter = 2\n",
    "max_inter = 4\n",
    "state_size = SETTINGS.STATE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# generate toy data\\ndata = []\\nfor user in range(num_users):\\n    num_interactions_now = random.randint(a=min_inter, b=max_inter)\\n    new = {\\n        \\'item_ids\\': torch.randint(low=1, high=num_items, size=(num_interactions_now, \\n        num_items_per_recom, recom_dim)).to(torch.float32),\\n        \\'timestamps\\': torch.sort(torch.FloatTensor(num_interactions_now).uniform_(0, 1.))[0].to(torch.float32),\\n        \\'interaction_types\\': torch.randint(low=0, high=num_interaction_types-1, \\n        size=(num_interactions_now, num_items_per_recom)),\\n        \"user_means\": torch.randn((state_size), requires_grad=True).to(torch.float32),\\n        \"user_vars_log\": torch.randn((state_size), requires_grad=True).to(torch.float32),\\n    }\\n    data.append(new)\\n\\n\\n# Create the dataset\\ndataset = CustomDataset(data)\\n\\n# Example usage with DataLoader\\ndataloader = DataLoader(dataset, batch_size=1, shuffle=True)# can only do batchsize 1\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# generate toy data\n",
    "data = []\n",
    "for user in range(num_users):\n",
    "    num_interactions_now = random.randint(a=min_inter, b=max_inter)\n",
    "    new = {\n",
    "        'item_ids': torch.randint(low=1, high=num_items, size=(num_interactions_now, \n",
    "        num_items_per_recom, recom_dim)).to(torch.float32),\n",
    "        'timestamps': torch.sort(torch.FloatTensor(num_interactions_now).uniform_(0, 1.))[0].to(torch.float32),\n",
    "        'interaction_types': torch.randint(low=0, high=num_interaction_types-1, \n",
    "        size=(num_interactions_now, num_items_per_recom)),\n",
    "        \"user_means\": torch.randn((state_size), requires_grad=True).to(torch.float32),\n",
    "        \"user_vars_log\": torch.randn((state_size), requires_grad=True).to(torch.float32),\n",
    "    }\n",
    "    data.append(new)\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "dataset = CustomDataset(data)\n",
    "\n",
    "# Example usage with DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)# can only do batchsize 1\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (list of dicts): Each dict contains 'timestamps', 'items', and 'labels' for a user.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_data = self.data[idx]\n",
    "        timestamps = user_data['timestamps']\n",
    "        items =  user_data['item_ids']\n",
    "        labels = user_data['interaction_types']\n",
    "        return timestamps, items, labels, user_data[\"user_means\"], user_data[\"user_vars_log\"], idx\n",
    "    \n",
    "    def Update_user_params(self, means_list, logvar_list, idx_list):\n",
    "        #means_list.requires_grad = True\n",
    "        #logvar_list.requires_grad = True\n",
    "        \n",
    "        #print(means_list)\n",
    "        \n",
    "        self.data[idx_list[0]][\"user_means\"] = means_list.tolist()\n",
    "        self.data[idx_list[0]][\"user_vars_log\"] = logvar_list.tolist()\n",
    "        #for means, logvar, idx in zip(means_list, logvar_list, idx_list):\n",
    "        #    self.data[idx][\"user_means\"] = means\n",
    "        #    self.data[idx][\"user_vars_log\"] = logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_dat, stg = process_dat.load_dat(paths.cw_stages[\\'output_new\\'][\\'train\\'], new_data=True)\\n\\nprint(stg)\\n\\ndef convert_string_to_double_list(s):\\n    return ast.literal_eval(s)\\n\\n# Apply the custom function\\ntrain_dat[\\'item_ids\\'] = train_dat[\\'item_ids\\'].apply(convert_string_to_double_list)\\ntrain_dat[\\'user_means\\'] = train_dat[\\'user_means\\'].apply(convert_string_to_double_list)\\ntrain_dat[\\'user_vars_log\\'] = train_dat[\\'user_vars_log\\'].apply(convert_string_to_double_list)\\ntrain_dat[\\'timestamps\\'] = train_dat[\\'timestamps\\'].apply(convert_string_to_double_list)\\ntrain_dat[\\'interaction_types\\'] = train_dat[\\'interaction_types\\'].apply(convert_string_to_double_list)\\n\\nprint(\"len: \", len(train_dat))\\nlist_of_dicts = train_dat.to_dict(orient=\\'records\\')\\n\\ntrain_dat.head()\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dataset from processed data\n",
    "'''\n",
    "train_dat, stg = process_dat.load_dat(paths.cw_stages['output_new']['train'], new_data=True)\n",
    "\n",
    "print(stg)\n",
    "\n",
    "def convert_string_to_double_list(s):\n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "# Apply the custom function\n",
    "train_dat['item_ids'] = train_dat['item_ids'].apply(convert_string_to_double_list)\n",
    "train_dat['user_means'] = train_dat['user_means'].apply(convert_string_to_double_list)\n",
    "train_dat['user_vars_log'] = train_dat['user_vars_log'].apply(convert_string_to_double_list)\n",
    "train_dat['timestamps'] = train_dat['timestamps'].apply(convert_string_to_double_list)\n",
    "train_dat['interaction_types'] = train_dat['interaction_types'].apply(convert_string_to_double_list)\n",
    "\n",
    "print(\"len: \", len(train_dat))\n",
    "list_of_dicts = train_dat.to_dict(orient='records')\n",
    "\n",
    "train_dat.head()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = torch.load(join(paths.dat, SETTINGS.filepaths_new['optimized_data']))\n",
    "list_of_dicts = checkpoint['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = CustomDataset(list_of_dicts[:50])\n",
    "# Example usage with DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)# can only do batchsize 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0417], dtype=torch.float64)\n",
      "tensor([69.3333], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def test_timestamps(dataloader):\n",
    "    smallest = float(\"inf\")\n",
    "    biggest = -1\n",
    "    for batch in dataloader:\n",
    "        timestamps, items, labels, means, var, idx = batch\n",
    "        last = timestamps[0]\n",
    "        smallest = min(smallest, last)\n",
    "        biggest = max(biggest, timestamps[-1])\n",
    "        for i in range(1,len(timestamps)):\n",
    "            if timestamps[i] <= last:\n",
    "                print(\"error, current: \", timestamps[i], \"\\tlast\", last)\n",
    "    print(smallest), print(biggest)\n",
    "    return biggest\n",
    "\n",
    "max_time = test_timestamps(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamps: [tensor([5.0833], dtype=torch.float64), tensor([6.3958], dtype=torch.float64), tensor([11.8125], dtype=torch.float64), tensor([28.0417], dtype=torch.float64), tensor([28.2708], dtype=torch.float64), tensor([40.1042], dtype=torch.float64), tensor([42.8542], dtype=torch.float64)]\n",
      "item_recom: [[tensor([161]), tensor([199]), tensor([279]), tensor([12]), tensor([37]), tensor([84]), tensor([74]), tensor([132]), tensor([161]), tensor([284]), tensor([250]), tensor([161]), tensor([144]), tensor([165]), tensor([161]), tensor([249]), tensor([9]), tensor([144]), tensor([35]), tensor([256]), tensor([205]), tensor([66]), tensor([297]), tensor([263]), tensor([57]), tensor([217]), tensor([205]), tensor([89]), tensor([290]), tensor([199]), tensor([62]), tensor([79]), tensor([84]), tensor([74]), tensor([132]), tensor([162]), tensor([165]), tensor([161]), tensor([249]), tensor([19]), tensor([144]), tensor([192]), tensor([250]), tensor([42]), tensor([165]), tensor([195]), tensor([191]), tensor([115]), tensor([256]), tensor([300]), tensor([286]), tensor([87]), tensor([132]), tensor([300]), tensor([126]), tensor([74]), tensor([144]), tensor([37]), tensor([174]), tensor([178]), tensor([53]), tensor([290]), tensor([113]), tensor([126]), tensor([138]), tensor([247]), tensor([109]), tensor([214]), tensor([249]), tensor([2]), tensor([150]), tensor([119]), tensor([300]), tensor([70]), tensor([229]), tensor([42]), tensor([161]), tensor([214]), tensor([295]), tensor([188]), tensor([279]), tensor([144]), tensor([279]), tensor([188]), tensor([162]), tensor([247]), tensor([286]), tensor([42]), tensor([279]), tensor([245]), tensor([295]), tensor([269]), tensor([227]), tensor([256]), tensor([212]), tensor([84]), tensor([19]), tensor([242]), tensor([18]), tensor([186]), tensor([258]), tensor([289]), tensor([102]), tensor([130]), tensor([130]), tensor([106]), tensor([262]), tensor([243]), tensor([102]), tensor([292]), tensor([210]), tensor([118]), tensor([98]), tensor([188]), tensor([74]), tensor([132]), tensor([162]), tensor([165]), tensor([161]), tensor([249]), tensor([263]), tensor([144]), tensor([31]), tensor([188]), tensor([70]), tensor([42]), tensor([161]), tensor([281]), tensor([290]), tensor([222]), tensor([296]), tensor([57]), tensor([289]), tensor([292]), tensor([51]), tensor([18]), tensor([77]), tensor([199]), tensor([50]), tensor([66]), tensor([53]), tensor([205]), tensor([217]), tensor([250]), tensor([256]), tensor([134]), tensor([84]), tensor([70]), tensor([87]), tensor([300]), tensor([250]), tensor([217]), tensor([286]), tensor([217]), tensor([149]), tensor([53]), tensor([247]), tensor([2]), tensor([161]), tensor([277]), tensor([242]), tensor([84]), tensor([74]), tensor([132]), tensor([162]), tensor([165]), tensor([161]), tensor([249]), tensor([144]), tensor([250]), tensor([300]), tensor([214]), tensor([205]), tensor([53]), tensor([284]), tensor([199]), tensor([62]), tensor([230]), tensor([77])], [tensor([49]), tensor([262]), tensor([106]), tensor([130]), tensor([35]), tensor([225]), tensor([102]), tensor([89]), tensor([292]), tensor([289]), tensor([106]), tensor([18]), tensor([53]), tensor([79]), tensor([84]), tensor([37]), tensor([162]), tensor([49]), tensor([262]), tensor([12]), tensor([149]), tensor([66]), tensor([74]), tensor([50]), tensor([66]), tensor([277]), tensor([217]), tensor([106]), tensor([79]), tensor([102]), tensor([289]), tensor([149]), tensor([242]), tensor([199]), tensor([62]), tensor([77]), tensor([2]), tensor([247]), tensor([115]), tensor([126]), tensor([259]), tensor([37]), tensor([161]), tensor([18]), tensor([242]), tensor([149]), tensor([9]), tensor([165]), tensor([242]), tensor([202]), tensor([4]), tensor([84]), tensor([217]), tensor([250]), tensor([279]), tensor([257]), tensor([163]), tensor([95]), tensor([74]), tensor([265]), tensor([263]), tensor([35]), tensor([217]), tensor([161]), tensor([144]), tensor([132]), tensor([300]), tensor([234]), tensor([297]), tensor([22]), tensor([244]), tensor([195]), tensor([297]), tensor([205]), tensor([297]), tensor([2]), tensor([130]), tensor([113]), tensor([144]), tensor([19]), tensor([74]), tensor([53]), tensor([263]), tensor([144]), tensor([295]), tensor([261]), tensor([155]), tensor([39]), tensor([249]), tensor([161]), tensor([250]), tensor([165]), tensor([162]), tensor([132]), tensor([94]), tensor([50]), tensor([144]), tensor([77]), tensor([217]), tensor([247]), tensor([79]), tensor([84]), tensor([74]), tensor([262]), tensor([49]), tensor([102]), tensor([289]), tensor([279]), tensor([262]), tensor([102]), tensor([300]), tensor([214]), tensor([114]), tensor([229]), tensor([279]), tensor([188]), tensor([286]), tensor([161]), tensor([28]), tensor([2]), tensor([77]), tensor([199]), tensor([199]), tensor([247]), tensor([149]), tensor([18]), tensor([242]), tensor([214]), tensor([132]), tensor([165]), tensor([165]), tensor([161]), tensor([102]), tensor([144]), tensor([144]), tensor([161]), tensor([300]), tensor([295]), tensor([205]), tensor([289]), tensor([53]), tensor([66]), tensor([50]), tensor([37]), tensor([194]), tensor([134]), tensor([279]), tensor([205]), tensor([229]), tensor([37]), tensor([279]), tensor([42]), tensor([84]), tensor([144]), tensor([199]), tensor([290]), tensor([89]), tensor([205]), tensor([217]), tensor([289]), tensor([292]), tensor([258]), tensor([18]), tensor([242]), tensor([62]), tensor([2]), tensor([57]), tensor([134]), tensor([53]), tensor([90]), tensor([62]), tensor([178]), tensor([247]), tensor([84]), tensor([74]), tensor([147]), tensor([98]), tensor([211]), tensor([169]), tensor([125]), tensor([158]), tensor([27]), tensor([269]), tensor([79]), tensor([235]), tensor([27]), tensor([263]), tensor([165]), tensor([132]), tensor([26]), tensor([161]), tensor([154]), tensor([269]), tensor([268])], [tensor([161]), tensor([249]), tensor([250]), tensor([165]), tensor([74]), tensor([162]), tensor([74]), tensor([249]), tensor([161]), tensor([19]), tensor([132]), tensor([144]), tensor([53]), tensor([84]), tensor([74]), tensor([84]), tensor([144]), tensor([178]), tensor([205]), tensor([195]), tensor([289]), tensor([19]), tensor([9]), tensor([165]), tensor([146]), tensor([132]), tensor([231]), tensor([244]), tensor([66]), tensor([53]), tensor([205]), tensor([217]), tensor([37]), tensor([217]), tensor([205]), tensor([53]), tensor([162]), tensor([66]), tensor([257]), tensor([95]), tensor([22]), tensor([120]), tensor([161]), tensor([250]), tensor([102]), tensor([84]), tensor([144]), tensor([84]), tensor([63]), tensor([295]), tensor([130]), tensor([234]), tensor([217]), tensor([161]), tensor([42]), tensor([188]), tensor([161]), tensor([249]), tensor([90]), tensor([66]), tensor([149]), tensor([2]), tensor([289]), tensor([292]), tensor([205]), tensor([53]), tensor([66]), tensor([297]), tensor([240]), tensor([50]), tensor([196]), tensor([106]), tensor([57]), tensor([102]), tensor([217]), tensor([205]), tensor([300]), tensor([53]), tensor([242]), tensor([296]), tensor([57]), tensor([205]), tensor([53]), tensor([66]), tensor([297]), tensor([50]), tensor([199]), tensor([62]), tensor([77]), tensor([217]), tensor([132]), tensor([62]), tensor([165]), tensor([199]), tensor([77]), tensor([162]), tensor([50]), tensor([138]), tensor([77]), tensor([62]), tensor([192]), tensor([284]), tensor([297]), tensor([258]), tensor([39]), tensor([258]), tensor([144]), tensor([242]), tensor([149]), tensor([50]), tensor([281]), tensor([289]), tensor([102]), tensor([113]), tensor([262]), tensor([106]), tensor([9]), tensor([130]), tensor([42]), tensor([262]), tensor([188]), tensor([2]), tensor([134]), tensor([77]), tensor([292]), tensor([70]), tensor([62]), tensor([289]), tensor([161]), tensor([19]), tensor([249]), tensor([161]), tensor([292]), tensor([113]), tensor([205]), tensor([217]), tensor([217]), tensor([205]), tensor([53]), tensor([66]), tensor([50]), tensor([199]), tensor([77]), tensor([134]), tensor([57]), tensor([2]), tensor([53]), tensor([66]), tensor([277]), tensor([199]), tensor([57]), tensor([199]), tensor([74]), tensor([132]), tensor([165]), tensor([2]), tensor([77]), tensor([290]), tensor([50]), tensor([217]), tensor([205]), tensor([53]), tensor([66]), tensor([297]), tensor([292]), tensor([279]), tensor([77]), tensor([297]), tensor([192]), tensor([9]), tensor([77]), tensor([130]), tensor([102]), tensor([289]), tensor([297]), tensor([292]), tensor([18]), tensor([242]), tensor([3]), tensor([158]), tensor([199]), tensor([242]), tensor([242]), tensor([258]), tensor([240]), tensor([35]), tensor([199]), tensor([102]), tensor([192]), tensor([153]), tensor([62]), tensor([263]), tensor([165]), tensor([132]), tensor([74]), tensor([84]), tensor([79]), tensor([161]), tensor([247]), tensor([130]), tensor([79]), tensor([144]), tensor([84]), tensor([89]), tensor([225]), tensor([284]), tensor([94]), tensor([162]), tensor([174]), tensor([12])], [tensor([247]), tensor([289]), tensor([102]), tensor([161]), tensor([165]), tensor([162]), tensor([126]), tensor([115]), tensor([292]), tensor([132]), tensor([184]), tensor([300]), tensor([214]), tensor([77]), tensor([190]), tensor([113]), tensor([300]), tensor([87]), tensor([70]), tensor([144]), tensor([219]), tensor([82]), tensor([256]), tensor([243]), tensor([130]), tensor([115]), tensor([249]), tensor([161]), tensor([2]), tensor([165]), tensor([258]), tensor([162]), tensor([165]), tensor([19]), tensor([132]), tensor([144]), tensor([19]), tensor([249]), tensor([161]), tensor([301]), tensor([217]), tensor([162]), tensor([161]), tensor([165]), tensor([205]), tensor([66]), tensor([297]), tensor([50]), tensor([249]), tensor([199]), tensor([77]), tensor([2]), tensor([144]), tensor([9]), tensor([19]), tensor([249]), tensor([161]), tensor([165]), tensor([144]), tensor([9]), tensor([162]), tensor([132]), tensor([74]), tensor([84]), tensor([62]), tensor([74]), tensor([153]), tensor([79]), tensor([132]), tensor([74]), tensor([84]), tensor([161]), tensor([134]), tensor([199]), tensor([277]), tensor([2]), tensor([235]), tensor([53]), tensor([289]), tensor([203]), tensor([271]), tensor([201]), tensor([165]), tensor([134]), tensor([195]), tensor([165]), tensor([249]), tensor([263]), tensor([144]), tensor([217]), tensor([205]), tensor([134]), tensor([174]), tensor([77]), tensor([50]), tensor([199]), tensor([66]), tensor([35]), tensor([249]), tensor([256]), tensor([161]), tensor([50]), tensor([165]), tensor([161]), tensor([234]), tensor([250]), tensor([77]), tensor([199]), tensor([205]), tensor([217]), tensor([74]), tensor([35]), tensor([256]), tensor([84]), tensor([70]), tensor([295]), tensor([249]), tensor([126]), tensor([161]), tensor([242]), tensor([205]), tensor([269]), tensor([273]), tensor([123]), tensor([37]), tensor([144]), tensor([9]), tensor([19]), tensor([217]), tensor([205]), tensor([53]), tensor([66]), tensor([62]), tensor([12]), tensor([12]), tensor([250]), tensor([242]), tensor([78]), tensor([58]), tensor([106]), tensor([190]), tensor([219]), tensor([211]), tensor([18]), tensor([162]), tensor([77]), tensor([50]), tensor([161]), tensor([249]), tensor([9]), tensor([149]), tensor([144]), tensor([242]), tensor([18]), tensor([74]), tensor([199]), tensor([292]), tensor([102]), tensor([219]), tensor([82]), tensor([262]), tensor([217]), tensor([205]), tensor([53]), tensor([66]), tensor([289]), tensor([242]), tensor([132]), tensor([134]), tensor([37]), tensor([42]), tensor([74]), tensor([259]), tensor([279]), tensor([229]), tensor([132]), tensor([188]), tensor([84]), tensor([247]), tensor([149]), tensor([49]), tensor([102]), tensor([289]), tensor([256]), tensor([289]), tensor([84]), tensor([115]), tensor([144]), tensor([300]), tensor([286]), tensor([126]), tensor([214]), tensor([149]), tensor([165]), tensor([2]), tensor([130]), tensor([106]), tensor([262]), tensor([243]), tensor([49]), tensor([18]), tensor([102]), tensor([73]), tensor([53]), tensor([84]), tensor([217]), tensor([300]), tensor([256]), tensor([250]), tensor([161]), tensor([161]), tensor([37]), tensor([161]), tensor([250]), tensor([70]), tensor([295]), tensor([214]), tensor([247]), tensor([74]), tensor([205]), tensor([57]), tensor([74]), tensor([84]), tensor([153]), tensor([234]), tensor([231]), tensor([130]), tensor([106]), tensor([262]), tensor([243]), tensor([165]), tensor([132]), tensor([199]), tensor([256]), tensor([217]), tensor([132]), tensor([62]), tensor([161]), tensor([165]), tensor([66]), tensor([297]), tensor([199]), tensor([192]), tensor([62]), tensor([234]), tensor([192]), tensor([50]), tensor([205]), tensor([66]), tensor([205]), tensor([217]), tensor([77]), tensor([35]), tensor([284]), tensor([134]), tensor([2]), tensor([19]), tensor([144]), tensor([35]), tensor([300]), tensor([217]), tensor([9]), tensor([53]), tensor([77]), tensor([66]), tensor([62]), tensor([249]), tensor([199]), tensor([50])], [tensor([279]), tensor([250]), tensor([77]), tensor([87]), tensor([289]), tensor([292]), tensor([258]), tensor([18]), tensor([242]), tensor([42]), tensor([247]), tensor([300]), tensor([192]), tensor([229]), tensor([82]), tensor([34]), tensor([219]), tensor([62]), tensor([300]), tensor([286]), tensor([214]), tensor([188]), tensor([199]), tensor([284]), tensor([161]), tensor([297]), tensor([217]), tensor([102]), tensor([84]), tensor([240]), tensor([66]), tensor([165]), tensor([166]), tensor([161]), tensor([250]), tensor([42]), tensor([144]), tensor([263]), tensor([198]), tensor([279]), tensor([162]), tensor([229]), tensor([188]), tensor([295]), tensor([249]), tensor([74]), tensor([114]), tensor([300]), tensor([144]), tensor([161]), tensor([132]), tensor([256]), tensor([147]), tensor([235]), tensor([256]), tensor([247]), tensor([144]), tensor([130]), tensor([53]), tensor([37]), tensor([130]), tensor([53]), tensor([205]), tensor([217]), tensor([300]), tensor([286]), tensor([295]), tensor([70]), tensor([279]), tensor([130]), tensor([106]), tensor([262]), tensor([18]), tensor([242]), tensor([289]), tensor([242]), tensor([106]), tensor([262]), tensor([243]), tensor([49]), tensor([102]), tensor([130]), tensor([18]), tensor([217]), tensor([62]), tensor([169]), tensor([234]), tensor([205]), tensor([66]), tensor([297]), tensor([149]), tensor([242]), tensor([64]), tensor([192]), tensor([133]), tensor([174]), tensor([18]), tensor([6]), tensor([178]), tensor([53]), tensor([126]), tensor([113]), tensor([290]), tensor([144]), tensor([242]), tensor([149]), tensor([102]), tensor([138]), tensor([243]), tensor([289]), tensor([258]), tensor([292]), tensor([102]), tensor([292]), tensor([289]), tensor([102]), tensor([262]), tensor([106]), tensor([149])], [tensor([262]), tensor([106]), tensor([77]), tensor([110]), tensor([62]), tensor([165]), tensor([42]), tensor([49]), tensor([247]), tensor([231]), tensor([84]), tensor([289]), tensor([161]), tensor([35]), tensor([102]), tensor([161]), tensor([263]), tensor([144]), tensor([165]), tensor([132]), tensor([110]), tensor([74]), tensor([165]), tensor([231]), tensor([217]), tensor([130]), tensor([18]), tensor([79]), tensor([242]), tensor([165]), tensor([249]), tensor([94]), tensor([300]), tensor([84]), tensor([192]), tensor([149]), tensor([295]), tensor([242]), tensor([205]), tensor([53]), tensor([66]), tensor([297]), tensor([50]), tensor([234]), tensor([199]), tensor([106]), tensor([77]), tensor([234]), tensor([57]), tensor([247]), tensor([250]), tensor([217]), tensor([161]), tensor([256]), tensor([242]), tensor([70]), tensor([195]), tensor([188]), tensor([247]), tensor([87]), tensor([286]), tensor([289]), tensor([256]), tensor([242]), tensor([262]), tensor([18]), tensor([243]), tensor([64]), tensor([300]), tensor([300]), tensor([169]), tensor([6]), tensor([205]), tensor([132]), tensor([214]), tensor([279]), tensor([247]), tensor([102]), tensor([205]), tensor([66]), tensor([279]), tensor([130]), tensor([300]), tensor([243]), tensor([18]), tensor([250]), tensor([161]), tensor([277]), tensor([49]), tensor([149]), tensor([292]), tensor([289]), tensor([102]), tensor([289]), tensor([229]), tensor([113]), tensor([262]), tensor([161]), tensor([144]), tensor([106]), tensor([149]), tensor([247]), tensor([149]), tensor([279]), tensor([62]), tensor([263]), tensor([89]), tensor([205]), tensor([217]), tensor([57]), tensor([199]), tensor([234]), tensor([50]), tensor([297]), tensor([130]), tensor([106]), tensor([262]), tensor([62]), tensor([102]), tensor([289]), tensor([126]), tensor([10]), tensor([292]), tensor([198]), tensor([11]), tensor([77]), tensor([73]), tensor([190]), tensor([8]), tensor([42]), tensor([217]), tensor([205]), tensor([211]), tensor([57]), tensor([53]), tensor([66]), tensor([138]), tensor([34]), tensor([259]), tensor([263]), tensor([114]), tensor([115]), tensor([229]), tensor([219]), tensor([191]), tensor([243]), tensor([115]), tensor([53]), tensor([290]), tensor([199]), tensor([62]), tensor([126]), tensor([144]), tensor([268]), tensor([295]), tensor([144]), tensor([9]), tensor([292]), tensor([35]), tensor([161]), tensor([165]), tensor([258]), tensor([286]), tensor([144]), tensor([261]), tensor([82]), tensor([297]), tensor([260]), tensor([217]), tensor([35]), tensor([12]), tensor([37]), tensor([161]), tensor([214]), tensor([295]), tensor([300]), tensor([229]), tensor([53]), tensor([135]), tensor([166]), tensor([242]), tensor([195]), tensor([102]), tensor([77]), tensor([242]), tensor([258]), tensor([292]), tensor([102]), tensor([243]), tensor([217]), tensor([62]), tensor([199]), tensor([130]), tensor([297]), tensor([243]), tensor([284]), tensor([234]), tensor([62]), tensor([217]), tensor([130]), tensor([53]), tensor([89]), tensor([66]), tensor([205])], [tensor([231]), tensor([165]), tensor([162]), tensor([132]), tensor([74]), tensor([84]), tensor([79]), tensor([234]), tensor([217]), tensor([199]), tensor([249]), tensor([188]), tensor([214]), tensor([300]), tensor([286]), tensor([279]), tensor([74]), tensor([132]), tensor([165]), tensor([161]), tensor([300]), tensor([279]), tensor([286]), tensor([161]), tensor([188]), tensor([19]), tensor([249]), tensor([161]), tensor([165]), tensor([162]), tensor([132]), tensor([74]), tensor([84]), tensor([263]), tensor([279]), tensor([188]), tensor([295]), tensor([300]), tensor([102]), tensor([62]), tensor([42]), tensor([144]), tensor([19]), tensor([249]), tensor([161]), tensor([42]), tensor([279]), tensor([229]), tensor([214]), tensor([144]), tensor([158]), tensor([144]), tensor([98]), tensor([211]), tensor([286]), tensor([125]), tensor([35]), tensor([279]), tensor([214]), tensor([74]), tensor([132]), tensor([295]), tensor([214]), tensor([165]), tensor([301]), tensor([169]), tensor([161]), tensor([249]), tensor([250]), tensor([144]), tensor([90]), tensor([188]), tensor([37]), tensor([229]), tensor([27]), tensor([205]), tensor([90]), tensor([217]), tensor([106]), tensor([247]), tensor([217]), tensor([62]), tensor([49]), tensor([9]), tensor([106]), tensor([147]), tensor([300]), tensor([134]), tensor([77]), tensor([199]), tensor([50]), tensor([297]), tensor([66]), tensor([279]), tensor([42]), tensor([247]), tensor([262]), tensor([144]), tensor([102]), tensor([243]), tensor([113]), tensor([262]), tensor([144]), tensor([12]), tensor([84]), tensor([300]), tensor([259]), tensor([256]), tensor([247]), tensor([70]), tensor([115]), tensor([290]), tensor([149]), tensor([289]), tensor([32]), tensor([135]), tensor([250]), tensor([132]), tensor([165]), tensor([115]), tensor([126]), tensor([259]), tensor([277]), tensor([161]), tensor([250]), tensor([256]), tensor([247]), tensor([70]), tensor([162]), tensor([161]), tensor([277]), tensor([126]), tensor([269]), tensor([161]), tensor([70]), tensor([42]), tensor([199]), tensor([217]), tensor([18]), tensor([242]), tensor([281]), tensor([188]), tensor([300]), tensor([205]), tensor([37]), tensor([53]), tensor([66]), tensor([50]), tensor([199]), tensor([77]), tensor([2]), tensor([134]), tensor([2]), tensor([87]), tensor([37]), tensor([144]), tensor([19]), tensor([249]), tensor([161]), tensor([263]), tensor([242]), tensor([263]), tensor([161]), tensor([292]), tensor([149]), tensor([18]), tensor([242]), tensor([234]), tensor([286]), tensor([300]), tensor([165]), tensor([161]), tensor([249]), tensor([9]), tensor([94]), tensor([144]), tensor([135]), tensor([149]), tensor([11]), tensor([289]), tensor([214]), tensor([289]), tensor([87]), tensor([19]), tensor([32]), tensor([190]), tensor([301]), tensor([74]), tensor([32]), tensor([135]), tensor([102]), tensor([49]), tensor([243]), tensor([300]), tensor([286]), tensor([214]), tensor([188]), tensor([229]), tensor([279]), tensor([57]), tensor([42]), tensor([144]), tensor([184]), tensor([262]), tensor([106]), tensor([35]), tensor([130]), tensor([278]), tensor([135]), tensor([247]), tensor([263])]]\n",
      "Labels: [[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0])], [tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0])], [tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0])], [tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0])], [tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0])], [tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0])], [tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0])]]\n",
      "means: [tensor([0.0431], dtype=torch.float64), tensor([-0.1170], dtype=torch.float64), tensor([-0.0166], dtype=torch.float64), tensor([0.3163], dtype=torch.float64), tensor([0.3602], dtype=torch.float64), tensor([-0.0831], dtype=torch.float64), tensor([0.0140], dtype=torch.float64), tensor([-0.2879], dtype=torch.float64)]\n",
      "log_var: [tensor([-0.2145], dtype=torch.float64), tensor([0.0386], dtype=torch.float64), tensor([0.0833], dtype=torch.float64), tensor([0.4553], dtype=torch.float64), tensor([-0.1556], dtype=torch.float64), tensor([0.0960], dtype=torch.float64), tensor([0.0055], dtype=torch.float64), tensor([0.1462], dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    timestamps, items, labels, means, var, idx = batch\n",
    "    print('Timestamps:', timestamps#, \"\\n dtype: \", timestamps.dtype\n",
    "          )\n",
    "    print('item_recom:', items#, \"\\n dtype: \", items.dtype\n",
    "          )\n",
    "    print('Labels:', labels#, \"\\n dtype: \", labels.dtype\n",
    "          )\n",
    "    print('means:', means#, \"\\n dtype: \", means.dtype\n",
    "          )\n",
    "    print('log_var:', var#, \"\\n dtype: \", var.dtype\n",
    "          )\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_1_path(model, user_state, timestamps, items, labels, loss_func, num_classes, max_time, device, teacher_forcing=True):\n",
    "    ''' expects batchsize of 1\n",
    "    '''\n",
    "    model.init_state(user_state)\n",
    "    loss_base = 0.\n",
    "    loss_intensity = 0.\n",
    "    curr_time = 1e-10# because time 0 is used sadly\n",
    "    N = len(timestamps)\n",
    "    max_div_by_N = max_time/N\n",
    "    for interaction_id in range(N):#[0]\n",
    "        h = (timestamps[interaction_id] - curr_time).float()\n",
    "        \n",
    "        intensity = model.eval_intensity(h)#loss on that\n",
    "        \n",
    "        #try:\n",
    "        model.evolve_state(h)\n",
    "        #except Exception as e:\n",
    "        #    print(\"delta: \",h , \"\\tnew: \", timestamps[interaction_id], \"\\told: \", curr_time)\n",
    "        #    print(e)\n",
    "\n",
    "        curr_time = h\n",
    "        # no intensity for now\n",
    "        y_pred = model.view_recommendations(items[interaction_id])# :,\n",
    "        y_true = torch.as_tensor(labels[interaction_id])# :,\n",
    "        y_true_onehot = nn.functional.one_hot(y_true, num_classes=num_classes).float()\n",
    "        \n",
    "        if teacher_forcing:\n",
    "            model.jump(y_true_onehot)\n",
    "        else:\n",
    "            model.jump(y_pred)\n",
    "        #loss += loss_func(y_true, y_pred)# for mse\n",
    "        y_true = y_true.squeeze(0)\n",
    "        y_pred = y_pred.squeeze(0)\n",
    "        y_pred = nn.functional.log_softmax(y_pred)\n",
    "        #print(\"true: \",y_true.shape, \"\\t predicted: \",y_pred.shape)\n",
    "        #print(torch.unique(y_true))\n",
    "        loss_base += loss_func(y_pred, y_true) # NLLL\n",
    "        loss_intensity += -torch.log(intensity) + max_div_by_N*intensity\n",
    "    \n",
    "    return loss_base, loss_intensity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter dicts\n",
    "width= 10\n",
    "user_state_dict = {\"model_hyp\": {\"layer_width\": [width, width, width]}}\n",
    "intensity_state_dict = {\"model_hyp\": {\"user_model_hyp\": {\"layer_width\": [width, width, 3],\n",
    "                                                         \"noise\": 0},\n",
    "                                          \"global_model_hyp\": {\"layer_width\": [width, 3]}}\n",
    "                            }\n",
    "interaction_state_dict = {\"model_hyp\": {\"layer_width\": [width, width ,width]}\n",
    "                            }\n",
    "jump_state_dict = {\"model_hyp\": {\"layer_width\": [width, width]}\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "hyperparameter_dict = {\"state_size\": state_size, \"state_model\": user_state_dict, \"num_interaction_outcomes\": num_interaction_types,\n",
    "                           \"intensity_model\": intensity_state_dict, \"num_recom\" : num_items_per_recom,\n",
    "                            \"recom_dim\":recom_dim, \"interaction_model\": interaction_state_dict,\n",
    "                            \"jump_model\": jump_state_dict}\n",
    "model = User_simmulation_Model(hyperparameter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "model.load_state_dict(torch.load(join(paths.dat, SETTINGS.filepaths_new['user_model'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_func = nn.functional.mse_loss\n",
    "loss_func = nn.NLLLoss()\n",
    "\n",
    "def kl_divergence(mu1, sigma1, mu2, sigma2):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence between two normal distributions N(mu1, sigma1^2) and N(mu2, sigma2^2).\n",
    "\n",
    "    Args:\n",
    "        mu1 (Tensor): Mean of the first distribution.\n",
    "        sigma1 (Tensor): Standard deviation of the first distribution.\n",
    "        mu2 (Tensor): Mean of the second distribution.\n",
    "        sigma2 (Tensor): Standard deviation of the second distribution.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: KL divergence.\n",
    "    \"\"\"\n",
    "    kl_div = torch.log(sigma2 / sigma1) + ((sigma1 ** 2 + (mu1 - mu2) ** 2) / (2 * sigma2 ** 2)) - 0.5\n",
    "    return kl_div\n",
    "\n",
    "def kl_divergence_to_standard_normal(mu, sigma):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence from a normal distribution N(mu, sigma^2) to the standard normal distribution N(0, 1).\n",
    "\n",
    "    Args:\n",
    "        mu (Tensor): Mean of the normal distribution.\n",
    "        sigma (Tensor): Standard deviation of the normal distribution.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: KL divergence.\n",
    "    \"\"\"\n",
    "    sigma2 = sigma ** 2\n",
    "    kl_div = 0.5 * (sigma2 + mu ** 2 - torch.log(sigma2) - 1)\n",
    "    return kl_div\n",
    "\n",
    "def kl_loss(mu, sigma):\n",
    "    return kl_divergence(mu, sigma, 0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_user_params(datadataloader, print_var = False, num_examples=5):\n",
    "    i = 0\n",
    "    for batch in dataloader:\n",
    "        timestamps, item_recom, labels, means, logvar, idx = batch\n",
    "        print(\"means: \", means)\n",
    "        if (print_var):\n",
    "            print(\"logvar: \", logvar)\n",
    "        i+=1\n",
    "        if i >= num_examples:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, dataloader,num_epochs, state_size, loss_func, loss_func_kl, optimizer, num_classes, \n",
    "            logger, max_time,lr_scheduler, warmup_scheduler,\n",
    "            kl_weight = 1, user_lr = None, log_step_size = 1, warmup_period=100):\n",
    "    model.to(device)\n",
    "    for epoch in tqdm(range(num_epochs)):  # Example: Number of epochs\n",
    "        loss_all, loss_base, loss_kl, loss_intensity = 0, 0, 0, 0\n",
    "        #print_user_params(dataloader)# see if values change\n",
    "        for batch in dataloader:\n",
    "            # Zero the gradients\n",
    "            \n",
    "            timestamps, item_recom, labels, means, logvar, idx = batch\n",
    "            timestamps, means, logvar = torch.as_tensor(timestamps).to(device).float(), \\\n",
    "                 torch.as_tensor(means).to(device).float(), torch.as_tensor(logvar).to(device).float()  #  item_recom, labels, = item_recom.to(device), labels.to(device),\n",
    "            \n",
    "            #timestamps, item_recom, labels, means, logvar = torch.as_tensor(timestamps).to(device), \\\n",
    "            #    torch.as_tensor(item_recom).to(device), torch.as_tensor(labels).to(device), \\\n",
    "            #    torch.as_tensor(means).to(device), torch.as_tensor(logvar).to(device)\n",
    "            means.requires_grad = True\n",
    "            logvar.requires_grad = True\n",
    "            #means.retain_grad()\n",
    "            #logvar.retain_grad()\n",
    "\n",
    "            variances = torch.exp(logvar)\n",
    "            user_state = means + variances*torch.randn((1, state_size))\n",
    "            #delta_from_previous = torch.cat([torch.zeros((timestamps.size(0),1)), timestamps[:,1:] - timestamps[:,:-1]], dim=1)\n",
    "            \n",
    "            curr_loss_base, curr_loss_intensity = train_1_path(model=model, user_state=user_state, timestamps=timestamps, items=item_recom, labels=labels,  \n",
    "                         loss_func=loss_func, max_time=max_time, num_classes=num_classes, device=device)\n",
    "            curr_loss_kl = kl_weight * torch.sum(loss_func_kl(means, variances))#.view(1,-1)\n",
    "            \n",
    "            curr_loss_all = curr_loss_kl + curr_loss_base+curr_loss_intensity\n",
    "            curr_loss_all.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            #logging\n",
    "            loss_all += curr_loss_all.item()\n",
    "            loss_base += curr_loss_base.item()\n",
    "            loss_kl += curr_loss_kl.item()\n",
    "            loss_intensity += curr_loss_intensity.item()\n",
    "            # maybe need to optim user_mean, user_var separate because of torch..\n",
    "            if user_lr:\n",
    "                torch.nn.utils.clip_grad_norm_(means, max_norm=1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(logvar, max_norm=1.0)\n",
    "                with torch.no_grad():\n",
    "                    means -= user_lr * means.grad\n",
    "                    logvar -= user_lr * logvar.grad  \n",
    "                #print(means)\n",
    "                means.zero_grad()\n",
    "                logvar.zero_grad()\n",
    "                dataloader.dataset.Update_user_params(means.detach(), logvar.detach(), idx)\n",
    "            \n",
    "            #print(\"lr: \",optimizer.param_groups[0]['lr'])\n",
    "            with warmup_scheduler.dampening():\n",
    "                if warmup_scheduler.last_step + 1 >= warmup_period:\n",
    "                    lr_scheduler.step()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "        if epoch % log_step_size == 0:\n",
    "            logger(loss_all, loss_base, loss_kl, loss_intensity)\n",
    "    logger(loss_all, loss_base, loss_kl, loss_intensity)# log at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging_func(loss_all, loss_base, loss_kl, loss_intensity):\n",
    "    print(\"loss_all: \", loss_all, \"\\tloss_base: \",loss_base, \"\\tloss_kl: \",loss_kl, \"\\tloss_intensity: \",loss_intensity, \"\\tlog of the loss: \", math.log(loss_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "warmup_period = len(dataset)\n",
    "num_steps = num_epochs*len(dataset) -warmup_period\n",
    "num_iter_til_first_restart = num_steps//2\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01,\n",
    "                        weight_decay=1e-8)\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=num_iter_til_first_restart, T_mult=1, eta_min=1e-4)\n",
    "\n",
    "warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means:  [tensor([0.1684], dtype=torch.float64), tensor([-0.2828], dtype=torch.float64), tensor([-0.0024], dtype=torch.float64), tensor([0.3760], dtype=torch.float64), tensor([0.0110], dtype=torch.float64), tensor([0.0295], dtype=torch.float64), tensor([-0.1031], dtype=torch.float64), tensor([-0.2597], dtype=torch.float64)]\n",
      "logvar:  [tensor([-0.2200], dtype=torch.float64), tensor([-0.0910], dtype=torch.float64), tensor([0.0476], dtype=torch.float64), tensor([-0.0762], dtype=torch.float64), tensor([-0.0448], dtype=torch.float64), tensor([0.0836], dtype=torch.float64), tensor([0.0206], dtype=torch.float64), tensor([0.2200], dtype=torch.float64)]\n",
      "means:  [tensor([0.1429], dtype=torch.float64), tensor([-0.2300], dtype=torch.float64), tensor([-0.0014], dtype=torch.float64), tensor([0.2801], dtype=torch.float64), tensor([0.1589], dtype=torch.float64), tensor([-0.0006], dtype=torch.float64), tensor([-0.0181], dtype=torch.float64), tensor([-0.1662], dtype=torch.float64)]\n",
      "logvar:  [tensor([-0.2179], dtype=torch.float64), tensor([-0.2234], dtype=torch.float64), tensor([-0.0101], dtype=torch.float64), tensor([-0.0605], dtype=torch.float64), tensor([0.0851], dtype=torch.float64), tensor([-0.0197], dtype=torch.float64), tensor([-0.0350], dtype=torch.float64), tensor([0.2017], dtype=torch.float64)]\n",
      "means:  [tensor([0.2094], dtype=torch.float64), tensor([-0.2476], dtype=torch.float64), tensor([-0.0978], dtype=torch.float64), tensor([0.4058], dtype=torch.float64), tensor([0.1194], dtype=torch.float64), tensor([-0.0137], dtype=torch.float64), tensor([-0.0319], dtype=torch.float64), tensor([-0.2288], dtype=torch.float64)]\n",
      "logvar:  [tensor([0.0375], dtype=torch.float64), tensor([-0.4195], dtype=torch.float64), tensor([0.0507], dtype=torch.float64), tensor([-0.0072], dtype=torch.float64), tensor([-0.1317], dtype=torch.float64), tensor([0.0060], dtype=torch.float64), tensor([-0.0003], dtype=torch.float64), tensor([0.2057], dtype=torch.float64)]\n",
      "means:  [tensor([0.1875], dtype=torch.float64), tensor([-0.1765], dtype=torch.float64), tensor([-0.1189], dtype=torch.float64), tensor([0.4246], dtype=torch.float64), tensor([0.0971], dtype=torch.float64), tensor([-0.1000], dtype=torch.float64), tensor([0.0378], dtype=torch.float64), tensor([-0.2706], dtype=torch.float64)]\n",
      "logvar:  [tensor([0.0016], dtype=torch.float64), tensor([0.0071], dtype=torch.float64), tensor([-0.1209], dtype=torch.float64), tensor([0.0097], dtype=torch.float64), tensor([-0.1623], dtype=torch.float64), tensor([0.1396], dtype=torch.float64), tensor([-0.0019], dtype=torch.float64), tensor([-0.0580], dtype=torch.float64)]\n",
      "means:  [tensor([-0.1181], dtype=torch.float64), tensor([0.0789], dtype=torch.float64), tensor([0.1148], dtype=torch.float64), tensor([-0.3930], dtype=torch.float64), tensor([-0.0232], dtype=torch.float64), tensor([-0.1402], dtype=torch.float64), tensor([0.0232], dtype=torch.float64), tensor([0.0650], dtype=torch.float64)]\n",
      "logvar:  [tensor([-0.0654], dtype=torch.float64), tensor([-0.0241], dtype=torch.float64), tensor([0.0458], dtype=torch.float64), tensor([0.0604], dtype=torch.float64), tensor([0.0700], dtype=torch.float64), tensor([0.0077], dtype=torch.float64), tensor([-0.0065], dtype=torch.float64), tensor([0.1262], dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "print_user_params(dataloader, print_var = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/thahit/anaconda3/envs/WW/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "train(model, dataloader=dataloader, num_epochs=num_epochs, device=device, loss_func=loss_func, \n",
    "                loss_func_kl=kl_divergence_to_standard_normal, kl_weight=1., user_lr=0.1,\n",
    "                optimizer=optimizer, lr_scheduler=lr_scheduler, num_classes=num_interaction_types, \n",
    "                logger=logging_func,warmup_period=warmup_period,\n",
    "                state_size=state_size,max_time=max_time, log_step_size=1, warmup_scheduler = warmup_scheduler,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means:  [tensor([0.0149], dtype=torch.float64), tensor([-0.0143], dtype=torch.float64), tensor([0.0468], dtype=torch.float64), tensor([0.0425], dtype=torch.float64), tensor([0.0993], dtype=torch.float64), tensor([0.0387], dtype=torch.float64), tensor([-0.0228], dtype=torch.float64), tensor([-0.0680], dtype=torch.float64)]\n",
      "logvar:  [tensor([0.1817], dtype=torch.float64), tensor([-0.0555], dtype=torch.float64), tensor([-0.0278], dtype=torch.float64), tensor([-0.3957], dtype=torch.float64), tensor([0.0319], dtype=torch.float64), tensor([-0.1186], dtype=torch.float64), tensor([0.1501], dtype=torch.float64), tensor([0.3147], dtype=torch.float64)]\n",
      "means:  [tensor([0.0865], dtype=torch.float64), tensor([-0.1777], dtype=torch.float64), tensor([0.0748], dtype=torch.float64), tensor([-0.1917], dtype=torch.float64), tensor([-0.0618], dtype=torch.float64), tensor([0.0318], dtype=torch.float64), tensor([0.0258], dtype=torch.float64), tensor([0.0558], dtype=torch.float64)]\n",
      "logvar:  [tensor([0.0162], dtype=torch.float64), tensor([-0.0764], dtype=torch.float64), tensor([-0.0083], dtype=torch.float64), tensor([-0.2836], dtype=torch.float64), tensor([0.0326], dtype=torch.float64), tensor([0.0097], dtype=torch.float64), tensor([0.0327], dtype=torch.float64), tensor([-0.0345], dtype=torch.float64)]\n",
      "means:  [tensor([-0.0916], dtype=torch.float64), tensor([0.1678], dtype=torch.float64), tensor([0.0806], dtype=torch.float64), tensor([-0.1010], dtype=torch.float64), tensor([0.1379], dtype=torch.float64), tensor([-0.0929], dtype=torch.float64), tensor([0.0877], dtype=torch.float64), tensor([0.0377], dtype=torch.float64)]\n",
      "logvar:  [tensor([0.0367], dtype=torch.float64), tensor([-0.2150], dtype=torch.float64), tensor([0.0098], dtype=torch.float64), tensor([0.0333], dtype=torch.float64), tensor([-0.1975], dtype=torch.float64), tensor([0.0259], dtype=torch.float64), tensor([-0.0390], dtype=torch.float64), tensor([0.0046], dtype=torch.float64)]\n",
      "means:  [tensor([-0.0382], dtype=torch.float64), tensor([0.0069], dtype=torch.float64), tensor([-0.0607], dtype=torch.float64), tensor([0.0900], dtype=torch.float64), tensor([0.1551], dtype=torch.float64), tensor([-0.1553], dtype=torch.float64), tensor([0.0447], dtype=torch.float64), tensor([-0.1774], dtype=torch.float64)]\n",
      "logvar:  [tensor([-0.2697], dtype=torch.float64), tensor([-0.0378], dtype=torch.float64), tensor([-0.1069], dtype=torch.float64), tensor([-0.3420], dtype=torch.float64), tensor([-0.0520], dtype=torch.float64), tensor([0.0276], dtype=torch.float64), tensor([-0.0699], dtype=torch.float64), tensor([0.1175], dtype=torch.float64)]\n",
      "means:  [tensor([0.0431], dtype=torch.float64), tensor([-0.1170], dtype=torch.float64), tensor([-0.0166], dtype=torch.float64), tensor([0.3163], dtype=torch.float64), tensor([0.3602], dtype=torch.float64), tensor([-0.0831], dtype=torch.float64), tensor([0.0140], dtype=torch.float64), tensor([-0.2879], dtype=torch.float64)]\n",
      "logvar:  [tensor([-0.2145], dtype=torch.float64), tensor([0.0386], dtype=torch.float64), tensor([0.0833], dtype=torch.float64), tensor([0.4553], dtype=torch.float64), tensor([-0.1556], dtype=torch.float64), tensor([0.0960], dtype=torch.float64), tensor([0.0055], dtype=torch.float64), tensor([0.1462], dtype=torch.float64)]\n",
      "means:  [tensor([0.1098], dtype=torch.float64), tensor([-0.2381], dtype=torch.float64), tensor([-0.0391], dtype=torch.float64), tensor([0.4226], dtype=torch.float64), tensor([0.0188], dtype=torch.float64), tensor([0.0730], dtype=torch.float64), tensor([-0.0927], dtype=torch.float64), tensor([-0.2449], dtype=torch.float64)]\n",
      "logvar:  [tensor([-0.2163], dtype=torch.float64), tensor([-0.0817], dtype=torch.float64), tensor([0.0725], dtype=torch.float64), tensor([0.1490], dtype=torch.float64), tensor([-0.0290], dtype=torch.float64), tensor([0.0149], dtype=torch.float64), tensor([-0.0159], dtype=torch.float64), tensor([-0.2935], dtype=torch.float64)]\n",
      "means:  [tensor([0.0514], dtype=torch.float64), tensor([-0.0856], dtype=torch.float64), tensor([-0.0359], dtype=torch.float64), tensor([0.2213], dtype=torch.float64), tensor([0.0791], dtype=torch.float64), tensor([0.0265], dtype=torch.float64), tensor([-0.0078], dtype=torch.float64), tensor([-0.1508], dtype=torch.float64)]\n",
      "logvar:  [tensor([-0.0473], dtype=torch.float64), tensor([0.0904], dtype=torch.float64), tensor([0.0182], dtype=torch.float64), tensor([0.2260], dtype=torch.float64), tensor([-0.1503], dtype=torch.float64), tensor([-0.0059], dtype=torch.float64), tensor([-0.0028], dtype=torch.float64), tensor([0.0747], dtype=torch.float64)]\n",
      "means:  [tensor([-0.0629], dtype=torch.float64), tensor([0.0297], dtype=torch.float64), tensor([0.0950], dtype=torch.float64), tensor([0.1668], dtype=torch.float64), tensor([0.2311], dtype=torch.float64), tensor([-0.0199], dtype=torch.float64), tensor([0.0541], dtype=torch.float64), tensor([-0.1571], dtype=torch.float64)]\n",
      "logvar:  [tensor([0.1149], dtype=torch.float64), tensor([-0.0245], dtype=torch.float64), tensor([-0.1048], dtype=torch.float64), tensor([-0.3196], dtype=torch.float64), tensor([-0.2237], dtype=torch.float64), tensor([0.1189], dtype=torch.float64), tensor([-0.0872], dtype=torch.float64), tensor([0.0221], dtype=torch.float64)]\n",
      "means:  [tensor([-0.1654], dtype=torch.float64), tensor([0.0151], dtype=torch.float64), tensor([0.0555], dtype=torch.float64), tensor([0.0179], dtype=torch.float64), tensor([0.0351], dtype=torch.float64), tensor([0.0753], dtype=torch.float64), tensor([-0.0776], dtype=torch.float64), tensor([-0.0882], dtype=torch.float64)]\n",
      "logvar:  [tensor([0.0842], dtype=torch.float64), tensor([-0.1430], dtype=torch.float64), tensor([-0.0566], dtype=torch.float64), tensor([0.0496], dtype=torch.float64), tensor([0.0012], dtype=torch.float64), tensor([0.0963], dtype=torch.float64), tensor([-0.2294], dtype=torch.float64), tensor([-0.1231], dtype=torch.float64)]\n",
      "means:  [tensor([0.1249], dtype=torch.float64), tensor([-0.1292], dtype=torch.float64), tensor([-0.0440], dtype=torch.float64), tensor([0.1998], dtype=torch.float64), tensor([0.0587], dtype=torch.float64), tensor([0.0197], dtype=torch.float64), tensor([-0.0262], dtype=torch.float64), tensor([-0.1064], dtype=torch.float64)]\n",
      "logvar:  [tensor([-0.1422], dtype=torch.float64), tensor([-0.1078], dtype=torch.float64), tensor([0.0986], dtype=torch.float64), tensor([0.0833], dtype=torch.float64), tensor([0.0237], dtype=torch.float64), tensor([-0.0201], dtype=torch.float64), tensor([0.0267], dtype=torch.float64), tensor([0.1995], dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "print_user_params(dataloader, print_var = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved_models_polimi/accordion/user_model.h5'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SETTINGS.filepaths['user_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "torch.save(model.state_dict(), join(paths.dat, SETTINGS.filepaths_new['user_model']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data(changes during training)\n",
    "torch.save({\n",
    "    'data': dataloader.dataset.data,\n",
    "}, join(paths.dat, SETTINGS.filepaths_new['optimized_data'])# 'saved_models_polimi/data.h5'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
