{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "1"
    }
   },
   "source": [
    "# Process ContentWise Data\n",
    "\n",
    "**Goal**: convert ContentWise dataset into pandas format that can be used to train simulator. Infrequent users and items are discarded, a column adding dynamic user state is added, and the dataset is split into train, validate, test users seen in training, and test for users unseen in training.\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "2"
    }
   },
   "outputs": [],
   "source": [
    "import paths\n",
    "%cd {paths.base}\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from simtrain import utils\n",
    "from simtrain import SETTINGS_POLIMI as SETTINGS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy\n",
    "from scipy import stats, sparse\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import time    # to be used in loop iterations\n",
    "import json\n",
    "\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = SETTINGS.NUMEXPR_MAX_THREADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "3"
    }
   },
   "outputs": [],
   "source": [
    "path_neg_imp = paths.cw_dat_paths['negative_impressions']\n",
    "path_pos_imp = paths.cw_dat_paths['positive_impressions']\n",
    "path_clicks = paths.cw_dat_paths['interactions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "3"
    }
   },
   "outputs": [],
   "source": [
    "df_neg = pd.read_csv(path_neg_imp)\n",
    "df_pos = pd.read_csv(path_pos_imp)\n",
    "df_int = pd.read_csv(path_clicks)\n",
    "len(df_neg), len(df_pos), len(df_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_neg), len(df_pos), len(df_int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "142"
    }
   },
   "source": [
    "### Next step: process positive impressions, negative impressions, and interactions into a single dataframe\n",
    "Warning: the next cell is memory intensive and will take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "18"
    }
   },
   "outputs": [],
   "source": [
    "def parse_items(s):\n",
    "    # convert string representation of items to list of int\n",
    "    return [int(s0) for s0 in s.split('[')[1].split(']')[0].split(' ') if len(s0)>0]\n",
    "\n",
    "# populate positive impressions:\n",
    "df_pos_merged = df_pos.merge(df_int, on='recommendation_id', how='inner')\n",
    "\n",
    "def convert_ms_days(t0, baseline):\n",
    "    return (t0 - baseline)/1000/3600/24\n",
    "\n",
    "start_time_ms = df_pos_merged.utc_ts_milliseconds.min()\n",
    "\n",
    "UID = []\n",
    "REC = []\n",
    "ITM = []\n",
    "TS = []\n",
    "RS = []\n",
    "\n",
    "print('processing interactions:')\n",
    "print(\"len(df_pos_merged): \", len(df_pos_merged))\n",
    "for ind,row in tqdm(df_pos_merged.iterrows()):\n",
    "    vs = parse_items(row.recommended_series_list)\n",
    "    c = row.recommendation_list_length\n",
    "    assert len(vs)==c\n",
    "    v1 = row.series_id\n",
    "    assert v1 in vs\n",
    "    t = convert_ms_days(row.utc_ts_milliseconds, start_time_ms)\n",
    "    # create set of new rows with same rec_id and user_id\n",
    "    UID += c*[row.user_id]\n",
    "    REC += c*[row.row_position]\n",
    "    ITM += vs\n",
    "    TS += c*[t]\n",
    "    RS += [((v0==v1) & (row.interaction_type==0)) for v0 in vs]\n",
    "\n",
    "    \n",
    "df = pd.DataFrame({'user_id':np.array(UID,dtype='int32'), \n",
    "                   'rec_id':np.array(REC,dtype='int32'),\n",
    "                   'item_id':np.array(ITM,dtype='int32'), \n",
    "                   't':np.array(TS,dtype='float32'),\n",
    "                    'reward':np.array(RS,dtype='float32')})\n",
    "    \n",
    "# group all positive events by user:\n",
    "user_sessions = df[df.reward>0].groupby('user_id')\n",
    "# keep users who have more than one play: \n",
    "valid_users = user_sessions.filter(lambda x: len(x)>1).user_id.unique()\n",
    "\n",
    "print('\\n\\nprocessing negative impressions:')\n",
    "print(\"df_neg: \",len(df_neg))\n",
    "\n",
    "# populate negative impressions:\n",
    "for ind, row in tqdm(df_neg.iterrows()):\n",
    "    uid = int(row.user_id)\n",
    "    if uid in valid_users:\n",
    "        vs = parse_items(row.recommended_series_list)\n",
    "        c = row.recommendation_list_length\n",
    "        # pick random true session for user and assign same timestamp:\n",
    "        t = user_sessions.get_group(uid).t.sample(n=1)\n",
    "        # create set of new rows with same rec_id and user_id\n",
    "        UID += c*[uid]\n",
    "        REC += c*[int(row.row_position)]\n",
    "        ITM += vs\n",
    "        TS += c*[t]\n",
    "        RS += c*[0]\n",
    "\n",
    "df_full = pd.DataFrame({'user_id':np.array(UID,dtype='int32'), \n",
    "                   'rec_id':np.array(REC,dtype='int32'),\n",
    "                   'item_id':np.array(ITM,dtype='int32'), \n",
    "                   't':np.array(TS,dtype='float32'),\n",
    "                    'reward':np.array(RS,dtype='float32')})\n",
    "    \n",
    "# init user state and cell id with dummary vars for now:\n",
    "df_full['user_state'] = ''\n",
    "df_full['cell_id'] = 1\n",
    "\n",
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(UID), len(REC), len(ITM), len(TS), len(RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UID2 = np.array(UID, dtype='int32')\n",
    "REC2 = np.array(REC, dtype='int32')\n",
    "ITM2 = np.array(ITM, dtype='int32')\n",
    "TS2 = np.array(TS, dtype='float32')\n",
    "RS2 = np.array(RS, dtype='float32')\n",
    "\n",
    "# Verify lengths\n",
    "assert len(UID2) == len(REC2) == len(ITM2) == len(TS2) == len(RS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.DataFrame({'user_id':UID, \n",
    "                   'rec_id':REC,\n",
    "                   'item_id':ITM, \n",
    "                   't':TS,\n",
    "                    'reward':RS,}\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['user_state'] = ''\n",
    "df_full['cell_id'] = 1\n",
    "\n",
    "df_full.to_csv(paths.cw_stages['intermediate_0'], compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv(paths.cw_stages['intermediate_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = 0\n",
    "for index, row in df_full.iterrows():\n",
    "    print(row)\n",
    "    found +=1\n",
    "    if found >20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ts_value(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.split()[1]  # Extract the numeric part\n",
    "    return float(value)\n",
    "\n",
    "clean_ts_value(\"7278048    27.36632\\nName: t, dtype: float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_reward(value):\n",
    "    if isinstance(value, bool):\n",
    "        return value\n",
    "    elif isinstance(value, int):\n",
    "        return bool(value)\n",
    "    elif isinstance(value, str):\n",
    "        return value.lower() in ['true', '1']\n",
    "    else:\n",
    "        return False  # Handle unexpected cases as False\n",
    "\n",
    "# Apply the function to the 'reward' column\n",
    "clean_reward(\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = 0\n",
    "for index, row in df_full.iterrows():\n",
    "    try:\n",
    "        # Try to convert the value to float\n",
    "        float_value = float(row['reward'])\n",
    "    except ValueError:\n",
    "        # If conversion fails, print the row\n",
    "        print(\"_____\"*50)\n",
    "        print(\"row index: \", index)\n",
    "        print(row)\n",
    "        found +=1\n",
    "        if found >20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['reward'] = df_full['reward'].apply(clean_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_t_column(value):\n",
    "    # Extract the second number in the string\n",
    "    if isinstance(value, str):\n",
    "        parts = value.split()\n",
    "        if len(parts) > 1:\n",
    "            return float(parts[1])\n",
    "    return float(value)\n",
    "\n",
    "# Apply the function to the 't' column\n",
    "df_full['t'] = df_full['t'].apply(clean_t_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['reward'] = df_full['reward'].astype(\"bool\")\n",
    "df_full['t'] = df_full['t'].astype(\"float32\")\n",
    "df_full['item_id'] = df_full['item_id'].astype(\"int32\")\n",
    "df_full['rec_id'] = df_full['rec_id'].astype(\"int32\")\n",
    "df_full['user_id'] = df_full['user_id'].astype(\"int32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['user_state'] = ''\n",
    "df_full['cell_id'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter bad \n",
    "#df_full['t'] = pd.to_numeric(df_full['t'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values in the column\n",
    "#df_full = df_full.dropna(subset=['t'])\n",
    "\n",
    "#df_full['reward'] = pd.to_numeric(df_full['reward'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values in the column\n",
    "#df_full = df_full.dropna(subset=['reward'])\n",
    "#len(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_full = pd.DataFrame({'user_id':np.array(UID,dtype='int32'), \n",
    "                   'rec_id':np.array(REC,dtype='int32'),\n",
    "                   'item_id':np.array(ITM,dtype='int32'), \n",
    "                   't':np.array(TS,dtype='float32'),\n",
    "                    'reward':np.array(RS,dtype='float32')})\n",
    "    \n",
    "# init user state and cell id with dummary vars for now:\n",
    "df_full['user_state'] = ''\n",
    "df_full['cell_id'] = 1\n",
    "\n",
    "df_full\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_extra = r'/media/thahit/Windows-SSD/Users/nicol/github/Warwick/ContentWiseImpressions/data/ContentWiseImpressions/CW10M-CSV/processed-0.6.csv.gz'\n",
    "path_extra = r'/home/thahit/github/Recommender_Sim/ContentWiseImpressions/data/ContentWiseImpressions/CW10M-CSV/processed-0.6.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "30"
    }
   },
   "outputs": [],
   "source": [
    "# save intermediary dataframe:\n",
    "#path_extra = r'C:\\Users\\nicol\\github\\Warwick\\ContentWiseImpressions\\data\\ContentWiseImpressions\\CW10M-CSV\\processed-0.5.csv.gz'\n",
    "df_full.to_csv(path_extra, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "39"
    }
   },
   "outputs": [],
   "source": [
    "#df_full = pd.read_csv(paths.cw_stages['intermediate_0'])\n",
    "df_full =pd.read_csv(path_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "48"
    }
   },
   "outputs": [],
   "source": [
    "df_full.rename(columns={'t':'time',\n",
    "                  'item_id':'original_action',\n",
    "                  'user_state':'original_state'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.drop(['Unnamed: 0.1', \"Unnamed: 0\"], axis=1, inplace=True)\n",
    "df_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['reward'] = df_full['reward'].astype(\"bool\")\n",
    "df_full['time'] = df_full['time'].astype(\"float32\")\n",
    "df_full['original_action'] = df_full['original_action'].astype(\"int32\")\n",
    "df_full['rec_id'] = df_full['rec_id'].astype(\"int32\")\n",
    "df_full['user_id'] = df_full['user_id'].astype(\"int32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_full.user_id.unique())#only 30k users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "146"
    }
   },
   "outputs": [],
   "source": [
    "# sub-sample users to make everything downstream runnable with a smaller machine \n",
    "# (can change SETTINGS parameter if no limits on computation):\n",
    "subsample_users = np.random.choice(df_full.user_id.unique(), 1000)\n",
    "df = df_full[df_full.user_id.isin(subsample_users)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "113"
    }
   },
   "outputs": [],
   "source": [
    "# count actions and discard those with less than 100 impressions or less than 5 streams:\n",
    "item_ic = df.groupby('original_action').original_action.count()\n",
    "item_sc = df[df.reward].groupby('original_action').original_action.count()\n",
    "keep_actions = item_ic[(item_ic >= 100) & (item_ic.index.isin(item_sc[item_sc>=5].index))].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "114"
    }
   },
   "outputs": [],
   "source": [
    "print('num. actions to keep', item_ic[item_ic>=10].index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "51"
    }
   },
   "outputs": [],
   "source": [
    "# map original action to sequential actions:\n",
    "\n",
    "# step 1. build vocab and inverse vocab of action_id\n",
    "vocab = np.sort(df[df.original_action.isin(keep_actions)].original_action.unique())\n",
    "inv_vocab = -np.ones(vocab.max()+1, dtype='int64') # -1 means item id is out of set\n",
    "for i,v in enumerate(vocab):\n",
    "    inv_vocab[v] = i\n",
    "\n",
    "print('num nonzero values:',len(np.where(inv_vocab>=0)[0]))\n",
    "print('len vocab',len(vocab))\n",
    "print('new vocab',vocab)\n",
    "\n",
    "# now add a new column with the sequential action ids\n",
    "action_map = pd.DataFrame({'original_action':vocab, 'action':np.arange(len(vocab),dtype='int64')})\n",
    "\n",
    "X1 = pd.merge(df, action_map, how='inner', on='original_action')\n",
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "53"
    }
   },
   "outputs": [],
   "source": [
    "# now add state representation column which is dynamic:\n",
    "\n",
    "def create_dynamic_states(X):\n",
    "    Xstream = X[X['reward']]\n",
    "    print('num. positive actions',Xstream.shape)\n",
    "    dynamic_states = [] # tracks user state changes, list of tuples [(user_id, time, state)]\n",
    "    format_state = lambda xs: ':'.join(map(str,xs))\n",
    "    for user_id,user_imp in X.groupby('user_id'):\n",
    "        current_state = []\n",
    "        ds = []\n",
    "        # iterate over streams that exist in impression data (if any) and update:\n",
    "        for name,imp in user_imp.sort_values('time')[['action','time','reward','rec_id']].iterrows():\n",
    "            title = int(imp[0])\n",
    "            time = imp[1]\n",
    "            reward = imp[2]\n",
    "            rec_id = int(imp[3])\n",
    "            ds.append((user_id, time, title, format_state(current_state), rec_id, reward))\n",
    "            current_state = current_state.copy()\n",
    "            if reward: current_state.append(title)\n",
    "        # only append dynamic states if there is a single stream for this user in the impressions\n",
    "        if len(current_state)>0:\n",
    "            dynamic_states += ds\n",
    "\n",
    "    users,times,titles,states,recids,rewards = zip(*dynamic_states)\n",
    "    return pd.DataFrame({'user_id':users, 'time':times, 'action':titles, 'state':states, 'rec_id':recids, 'reward':rewards})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdyn_ = create_dynamic_states(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "55"
    }
   },
   "outputs": [],
   "source": [
    "# might be bugged\n",
    "\n",
    "# parallelize across half as many cores because memory intensive:\n",
    "cores = max(1, int(int(os.environ['NUMEXPR_MAX_THREADS'])/2))\n",
    "#cores = int(os.environ['NUMEXPR_MAX_THREADS'])\n",
    "# need to process each user state at the time of every impression, use parallelism to solve faster:\n",
    "#Xdyn_ = utils.parallelize_fnc_groups(create_dynamic_states, X1, None, 'user_id', cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "84"
    }
   },
   "outputs": [],
   "source": [
    "Xdyn_.to_csv(paths.cw_stages['intermediate_1'], compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "122"
    }
   },
   "outputs": [],
   "source": [
    "Xdyn_ = pd.read_csv(paths.cw_stages['intermediate_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "59"
    }
   },
   "outputs": [],
   "source": [
    "# annotate original dataset with Xdyn:\n",
    "X2 = pd.merge(X1, Xdyn_, on=['user_id','time','action'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "71"
    }
   },
   "outputs": [],
   "source": [
    "# look histogram of delta t between visits to decide whether further sessionization is needed:\n",
    "# take subset of 500 users, do on individual level:\n",
    "dt = []\n",
    "n_sessions = []\n",
    "n_sample_users = 100\n",
    "HOURS_IN_DAY = 24\n",
    "sub_users = np.random.choice(X1.user_id.unique(), n_sample_users)\n",
    "for u in sub_users:\n",
    "    t0 = X1[X1.user_id == u].sort_values('time').time.values\n",
    "    dt_ = HOURS_IN_DAY*(t0[1:] - t0[:-1])\n",
    "    n = len(np.unique(dt_[dt_>1.0]))\n",
    "    n_sessions.append(1+n)\n",
    "    dt += list(dt_[(dt_ > 0) & (dt_ < 1)])\n",
    "plt.hist(dt,bins=100)\n",
    "print('n sesions per user over 1 hour apart', sum(n_sessions)/n_sample_users)\n",
    "plt.figure()\n",
    "plt.hist(n_sessions,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "92"
    }
   },
   "outputs": [],
   "source": [
    "X3 = Xdyn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "95"
    }
   },
   "outputs": [],
   "source": [
    "# bucketize time into 30 min time slots:\n",
    "X3['round_time'] = np.round(X3.time*48)/48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "96"
    }
   },
   "outputs": [],
   "source": [
    "Xus = X3.groupby('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "97"
    }
   },
   "outputs": [],
   "source": [
    "session_count = Xus.apply(lambda g: g.groupby('round_time').ngroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "98"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(session_count, bins=100)\n",
    "plt.xlabel('number of sessions')\n",
    "plt.ylabel('frequency')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "103"
    }
   },
   "outputs": [],
   "source": [
    "print('proportion of users with less than 5 sessions', \n",
    "      session_count[session_count < 5].shape[0] / session_count.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "105"
    }
   },
   "outputs": [],
   "source": [
    "# keep only those users with 5 or more sessions (discretized by 30 min slots)\n",
    "keep_users = session_count[session_count >= 5].index\n",
    "keep_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "107"
    }
   },
   "outputs": [],
   "source": [
    "X4 = X3[X3.user_id.isin(keep_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "80"
    }
   },
   "outputs": [],
   "source": [
    "# now split data into last 28 days test, train-validation-test 50-20-30, \n",
    "# remove users with no sessions in their respective quandrants\n",
    "\n",
    "def split_dataset(X_, prop_train = 0.7, prop_test = 0.3, TEST_TIME_DAYS = 28, seed = 0):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    n_users = X_.user_id.nunique()\n",
    "    user_set = X_.user_id.unique()\n",
    "    \n",
    "    train_user_ids = np.random.choice(np.arange(n_users,dtype='int64'), int(n_users*prop_train), replace=False)\n",
    "    train_users = np.zeros(n_users, dtype='bool') #np.random.binomial(1, prop_train, size=n_users).astype('bool')\n",
    "    train_users[train_user_ids] = True\n",
    "    #val_users = (1-train_users)*np.random.binomial(1, prop_val/prop_train, size=n_users).astype('bool')\n",
    "    test_users = (1 - train_users).astype('bool')\n",
    "    \n",
    "    print('stats: prop train users, test users', train_users.sum()/n_users, test_users.sum()/n_users)\n",
    "    print('n_train_users',train_users.sum(), 'n_test_users',test_users.sum(), 'n_users', n_users)\n",
    "\n",
    "    # now split by time and save to disk:\n",
    "    max_time = X_.time.max()\n",
    "    min_time = X_.time.min()\n",
    "    print(min_time)\n",
    "    #assert np.abs(min_time) < 1e-8# don't get why this is needed\n",
    "\n",
    "    test_time = max_time - TEST_TIME_DAYS\n",
    "    \n",
    "    X_train = X_[(X_.time < test_time) & (X_.user_id.isin(user_set[train_users]))]\n",
    "    X_val = X_[(X_.time < test_time) & (X_.user_id.isin(user_set[test_users]))]\n",
    "    X_test_seen_users = X_[(X_.time >= test_time) & (X_.user_id.isin(user_set[train_users]))]\n",
    "    X_test_unseen_users = X_[(X_.time >= test_time) & (X_.user_id.isin(user_set[test_users]))]\n",
    "    \n",
    "    N = X_.shape[0]\n",
    "    print('data sizes as proportion of all data:', 'X_train', X_train.shape[0]/N, 'X_val', X_val.shape[0]/N, \n",
    "         'X_test_seen_users', X_test_seen_users.shape[0]/N, 'X_test_unseen_users', X_test_unseen_users.shape[0]/N)\n",
    "    print('total imp check total', N, ': ', X_train.shape[0] + X_val.shape[0] + X_test_seen_users.shape[0] + X_test_unseen_users.shape[0])\n",
    "    return [X_train, X_val, X_test_seen_users, X_test_unseen_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "111"
    }
   },
   "outputs": [],
   "source": [
    "# save final outputs of processing to split files:\n",
    "\n",
    "file_names = [paths.cw_stages['output']['train'],\n",
    "             paths.cw_stages['output']['validate'],\n",
    "             paths.cw_stages['output']['test-seen'],\n",
    "             paths.cw_stages['output']['test-unseen']]\n",
    "\n",
    "Xs = split_dataset(X4, seed=42)\n",
    "\n",
    "for x,fn in zip(Xs, file_names):\n",
    "    x.to_csv(fn, compression='gzip')\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4.user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "112"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sanity check on state encoding: pick arbitrary users and see if their state matches their actions over time\n",
    "\n",
    "def display_hist(uids):\n",
    "    for uid in uids:\n",
    "        print('uid',uid,'\\n',X4[(X4.user_id == uid) & (X4.reward > 0)])\n",
    "        print('\\n\\n')\n",
    "    \n",
    "display_hist([188, 42148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "nterop": {
   "seedId": "152"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
