{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "# Add the parent directory to the sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "#import simtrain\n",
    "from simtrain.sim_models_new import User_simmulation_Model, Conditioned_User_simmulation_Model, Toy_intensity_Comparer, all_in_one_model\n",
    "\n",
    "from simtrain import SETTINGS_POLIMI as SETTINGS\n",
    "from simtrain import explore_models, process_dat\n",
    "from simtrain.train import train_function_approx_multiple_variational, train_density_multiple_variational_sorted\n",
    "\n",
    "import simtrain.utils as utils\n",
    "from simtrain.Dataset import CustomDataset, TimestepFrequencyDataset\n",
    "from simtrain.train import train, train_with_negatives\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import ast\n",
    "\n",
    "import paths\n",
    "from os.path import join\n",
    "import pytorch_warmup as warmup\n",
    "from functools import partial\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_items = 7\n",
    "#num_items_per_recom = 2\n",
    "num_interaction_types = 2\n",
    "recom_dim = 1\n",
    "#num_users = 11\n",
    "#min_inter = 2\n",
    "#max_inter = 4\n",
    "state_size = SETTINGS.STATE_SIZE\n",
    "subset = 2 # make data smaller\n",
    "experiment_name = \"intensity\"\n",
    "num_negatives = 150\n",
    "conditioned=False\n",
    "kl_weight=.01\n",
    "state_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old\n",
    "Data generation that is not in use currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset from processed data\n",
    "if True:\n",
    "    train_dat, stg = process_dat.load_dat(paths.cw_stages['output_new']['train'], new_data=True)\n",
    "\n",
    "    print(stg)\n",
    "\n",
    "    def convert_string_to_double_list(s):\n",
    "        return ast.literal_eval(s)\n",
    "\n",
    "    # Apply the custom function\n",
    "    train_dat['item_ids'] = train_dat['item_ids'].apply(convert_string_to_double_list)\n",
    "    train_dat['user_means'] = train_dat['user_means'].apply(convert_string_to_double_list)\n",
    "    train_dat['user_vars_log'] = train_dat['user_vars_log'].apply(convert_string_to_double_list)\n",
    "    train_dat['timestamps'] = train_dat['timestamps'].apply(convert_string_to_double_list)\n",
    "    train_dat['interaction_types'] = train_dat['interaction_types'].apply(convert_string_to_double_list)\n",
    "\n",
    "    print(\"len: \", len(train_dat))\n",
    "    list_of_dicts = train_dat.to_dict(orient='records')\n",
    "    list_of_dicts = list_of_dicts[:subset]\n",
    "    train_dat.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint = torch.load(join(paths.dat, SETTINGS.rootpaths['models'],\n",
    "#                             experiment_name, \"data.h5\"))\n",
    "#list_of_dicts = checkpoint['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adjust_hidden_dim(data_dict, state_size):\n",
    "    # just to jumstart experiments should be deleted at the end\n",
    "    for row in data_dict:\n",
    "        if row[\"user_means\"] > state_size:\n",
    "            row[\"user_means\"] = row[\"user_means\"][:state_size]\n",
    "            row[\"user_vars_log\"] = row[\"user_vars_log\"][:state_size]\n",
    "        elif row[\"user_means\"] > state_size:\n",
    "            pass# dont care\n",
    "\n",
    "#adjust_hidden_dim(list_of_dicts, state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(list_of_dicts[:subset]) # [:30]\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)# can only do batchsize 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(dataloader):\n",
    "    smallest = float(\"inf\")\n",
    "    biggest = -1\n",
    "    count_classes = [0 for _ in range(num_interaction_types)]\n",
    "    for batch in dataloader:\n",
    "        timestamps, items, labels, means, var, idx = batch\n",
    "        last = timestamps[0]\n",
    "        smallest = min(smallest, last)\n",
    "        biggest = max(biggest, timestamps[-1])\n",
    "        for i in range(1,len(timestamps)):\n",
    "            if timestamps[i] <= last:\n",
    "                print(\"error, current: \", timestamps[i], \"\\tlast\", last)\n",
    "        \n",
    "        for i in range(num_interaction_types):\n",
    "            for row in labels:\n",
    "                row=torch.as_tensor(row)\n",
    "                count = torch.sum(row == i)\n",
    "                count_classes[i] += count\n",
    "\n",
    "    smallest, biggest= int(smallest), float(biggest)\n",
    "    print(smallest), print(biggest)\n",
    "    for i in range(num_interaction_types):\n",
    "        print(f\"number of interactions of type {i} = {count_classes[i]}\")\n",
    "    return biggest\n",
    "\n",
    "max_time = test_data(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for batch in dataloader:\n",
    "    timestamps, items, labels, means, var, idx = batch\n",
    "    print('Timestamps:', timestamps#, \"\\n dtype: \", timestamps.dtype\n",
    "          )\n",
    "    print('item_recom:', items#, \"\\n dtype: \", items.dtype\n",
    "          )\n",
    "    print('Labels:', labels#, \"\\n dtype: \", labels.dtype\n",
    "          )\n",
    "    print('means:', means#, \"\\n dtype: \", means.dtype\n",
    "          )\n",
    "    print('log_var:', var#, \"\\n dtype: \", var.dtype\n",
    "          )\n",
    "    break\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sorted = False\n",
    "batchsize = 64  # needs to be 1 for ode\n",
    "centered_intervals = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset from processed data\n",
    "if True:\n",
    "    train_dat, stg = process_dat.load_dat(paths.cw_stages['output_new']['train'], new_data=True)\n",
    "\n",
    "    print(stg)\n",
    "\n",
    "    def convert_string_to_double_list(s):\n",
    "        return ast.literal_eval(s)\n",
    "\n",
    "    # Apply the custom function\n",
    "    train_dat['item_ids'] = train_dat['item_ids'].apply(convert_string_to_double_list)\n",
    "    train_dat['user_means'] = train_dat['user_means'].apply(convert_string_to_double_list)\n",
    "    train_dat['user_vars_log'] = train_dat['user_vars_log'].apply(convert_string_to_double_list)\n",
    "    train_dat['timestamps'] = train_dat['timestamps'].apply(convert_string_to_double_list)\n",
    "    train_dat['interaction_types'] = train_dat['interaction_types'].apply(convert_string_to_double_list)\n",
    "\n",
    "    print(\"len: \", len(train_dat))\n",
    "    list_of_dicts = train_dat.to_dict(orient='records')\n",
    "    list_of_dicts = list_of_dicts[:subset]\n",
    "    train_dat.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "\n",
    "dataloader_list = []\n",
    "#steps_per_epoch = 0 \n",
    "\n",
    "for user in list_of_dicts:\n",
    "    timestamps = user[\"timestamps\"]\n",
    "    means, logvars = user[\"user_means\"], user[\"user_vars_log\"]\n",
    "    extras = {\"user_id\":user[\"user_id\"]}\n",
    "    #print(user[\"user_id\"])\n",
    "    reaction_ratio = [np.mean(np.array(row)) for row in user[\"interaction_types\"]]\n",
    "    dataset_for_user = TimestepFrequencyDataset(timestamps, num_random_points=num_negatives, \n",
    "                            reaction_ratio=reaction_ratio)\n",
    "\n",
    "    dataloader = DataLoader(dataset_for_user, batch_size=batchsize, shuffle=not train_sorted\n",
    "                        )\n",
    "    dataloader_list.append([dataloader, means, logvars, extras])\n",
    "    #steps_per_epoch += len(dataset) // batchsize  \n",
    "    #if len(dataset) % batchsize != 0:\n",
    "    #    steps_per_epoch += 1\n",
    "steps_per_epoch = 64# for 64b 80u, for 16b 20u: 221, for 64b u20: 63, \n",
    "\n",
    "if batchsize==1 and train_sorted:\n",
    "    steps_per_epoch = 20\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(len(data[0]) for data in dataloader_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids= [188,491,561,670,749,800,879,931,1372,1480,1622,2118,2249,2290,2725,2808,2839,2905,2920, 3059]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = []\n",
    "for user in list_of_dicts:\n",
    "    timestamps = user[\"timestamps\"]\n",
    "    means, logvars = user[\"user_means\"], user[\"user_vars_log\"]\n",
    "    reaction_ratio = [np.mean(np.array(row)) for row in user[\"interaction_types\"]]\n",
    "    extras = {\"user_id\":user[\"user_id\"]}\n",
    "    path_list.append([user[\"timestamps\"], means, logvars, reaction_ratio, extras])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density\n",
    "Predict the density instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 64\n",
    "model_type = \"simple\"\n",
    "use_variational_nn = True\n",
    "state_consistancy_training = False\n",
    "use_jump = False\n",
    "timecheat = False\n",
    "\n",
    "intensity = {\"model_hyp\": {\"layer_width\": [width for _ in range(3)],\n",
    "                           \"bayesian\": use_variational_nn,\n",
    "                           }}\n",
    "state_dict = {\"model_hyp\": {\"layer_width\": [width for _ in range(4)],\n",
    "                            \"noise\": 0, \"bayesian\": use_variational_nn,},\n",
    "            }\n",
    "jump_dict= {\"model_hyp\": {\"layer_width\": [8, 8]}\n",
    "            }\n",
    "hyperparameter_dict = {\"state_size\": state_size, \"state_model\": state_dict, \n",
    "        \"intensity_model\": intensity, \"state_model_type\": model_type, # simple\n",
    "        #\"jump_model\": jump_dict,\n",
    "        \"time_embedding_size\" :32, \"max_freq\": 30,\n",
    "        }\n",
    "if use_jump:\n",
    " hyperparameter_dict[\"jump_model\"] =jump_dict\n",
    "model = Toy_intensity_Comparer(hyperparameter_dict)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simtrain.utils import weighted_mse_loss\n",
    "from simtrain.train import train_density_multiple_variational\n",
    "\n",
    "loss = partial(weighted_mse_loss, weight_pos=10)\n",
    "loss_func_kl=partial(utils.kl_divergence, mu2=0, sigma2=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 201\n",
    "warmup_period = steps_per_epoch\n",
    "num_steps = num_epochs*steps_per_epoch - warmup_period\n",
    "num_iter_til_first_restart = (num_steps + 1)//2\n",
    "# restarts seem to ne be great most of the time here\n",
    "user_lr = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=user_lr,\n",
    "                        weight_decay=1e-10)\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=num_iter_til_first_restart, T_mult=1, eta_min=1e-5)\n",
    "\n",
    "warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period)\n",
    "num_iter_til_first_restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_sorted:\n",
    "    train_density_multiple_variational_sorted(model, dataloader_list, criterion=weighted_mse_loss, \n",
    "            state_size=state_size, user_lr_decay=0.995, train_bayesian_weight= 0.1,\n",
    "            optimizer=optimizer, num_epochs=num_epochs, warmup_scheduler=warmup_scheduler,\n",
    "            loss_func_kl=loss_func_kl, kl_weight=kl_weight, user_lr=.02, device=device, \n",
    "            loss_print_interval=25, warmup_period=warmup_period, lr_scheduler=lr_scheduler,\n",
    "            logging_shift = 1, model_type=model_type, use_jump=use_jump)\n",
    "else:\n",
    "    results =train_density_multiple_variational(model, dataloader_list, criterion=weighted_mse_loss, \n",
    "            state_size=state_size, user_lr_decay=0.995, train_bayesian_weight= 0.1,\n",
    "            state_consistancy_training=state_consistancy_training, consistancy_weight=.5,\n",
    "            optimizer=optimizer, num_epochs=num_epochs, warmup_scheduler=warmup_scheduler,\n",
    "            loss_func_kl=loss_func_kl, kl_weight=kl_weight, user_lr=.02, device=device, \n",
    "            loss_print_interval=25, warmup_period=warmup_period, lr_scheduler=lr_scheduler,\n",
    "            logging_shift = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycycyc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11, 10, 4, 0, 13, 15\n",
    "# good ones: 4=491? , 13=2290, \n",
    "selected_user = 3\n",
    "utils.generate_density_plot(selected_user, model, model_type, dataloader_list, \n",
    "            use_variational_nn=False, train_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for selected_user in range(len(dataloader_list)):\n",
    "    utils.generate_density_plot(selected_user, model, model_type, dataloader_list, \n",
    "                    use_variational_nn=False, train_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ugly version, pront variance in separate plot, is decieving\n",
    "fig, axs = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "selected_user = 11\n",
    "dataloader, variational_means, variational_logvar, extras = dataloader_list[selected_user]\n",
    "print(extras[\"user_id\"])\n",
    "dataset = dataloader.dataset\n",
    "x_range = np.linspace(0, 71, 71*2)  # Adjust the range as needed\n",
    "x_range_tensor = torch.tensor(x_range, dtype=torch.float32).unsqueeze(1)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    if model_type == \"ode\":\n",
    "        raise NotImplementedError\n",
    "        state = torch.zeros((1, state_size))\n",
    "\n",
    "        predictions = []\n",
    "        for el in x_range:\n",
    "            out = model(state, el)\n",
    "            predictions.append(out[0])\n",
    "    else:\n",
    "        if use_variational_nn:\n",
    "            predictions_list = []\n",
    "            for _ in range(20):\n",
    "                state = torch.tensor(variational_means).repeat(len(x_range),1)\n",
    "                predictions, states = model(state, x_range_tensor, return_new_state=True)\n",
    "                predictions = predictions.numpy()\n",
    "                predictions_list.append(predictions)\n",
    "                #print(\"variance of states: \", torch.var(states))\n",
    "            predictions_list = np.stack(predictions_list)\n",
    "            predictions = np.mean(predictions_list, axis=0)\n",
    "            var = np.var(predictions_list, axis=0)\n",
    "            cond = (0.1>var) | (var <.01)\n",
    "            predictions = np.where(cond.any() , predictions, 0)\n",
    "\n",
    "\n",
    "        else:\n",
    "            state = torch.tensor(variational_means).repeat(len(x_range),1)\n",
    "            predictions, states = model(state, x_range_tensor, return_new_state=True)\n",
    "            predictions = predictions.numpy()\n",
    "            print(\"variance of states: \", torch.var(states))\n",
    "\n",
    "print(f\"area: {np.sum(predictions)*(72/200)}\")\n",
    "# Plot the results\n",
    "plt.plot(x_range, predictions, label='Model Predictions')\n",
    "\n",
    "if use_variational_nn:\n",
    "    axs[0].plot(x_range, var, label='variance', color=\"green\")\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    x_pos= sample['timestep'].item()\n",
    "    height = torch.where(sample['frequency']>0, sample['frequency'], .1).item()\n",
    "    plt.plot([x_pos, x_pos], [0, height], linestyle='--', color='red')\n",
    "\n",
    "plt.plot([0, 0], [0, 0], color='red', linestyle='--', alpha=1.0, label='Data Points')\n",
    "\n",
    "#plt.scatter(x_train.numpy(), y_train.numpy(), color='red', label='Training Data')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('Model Predictions Over Input Range')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize embeddings\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [data[1] for data in dataloader_list]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(data)\n",
    "\n",
    "# Plot the reduced data\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c='blue', alpha=0.5)\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('2D PCA Projection of Embedding space')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot with variances\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "means = [data[1] for data in dataloader_list]\n",
    "covariances = [np.exp(data[2])*np.eye(state_size) for data in dataloader_list]\n",
    "\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(means)))\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced_means = pca.fit_transform(means)\n",
    "\n",
    "def project_covariance(cov, pca):\n",
    "    return pca.components_ @ (cov ) @ pca.components_.T\n",
    "\n",
    "reduced_covariances = [project_covariance(cov, pca) for cov in covariances]\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots()\n",
    "i =0\n",
    "for mean, cov in zip(reduced_means, reduced_covariances):\n",
    "    # Plot the mean\n",
    "    ax.scatter(mean[0], mean[1], color=colors[i])\n",
    "    \n",
    "    # Calculate and plot the 2D Gaussian ellipse\n",
    "    vals, vecs = np.linalg.eigh(cov)\n",
    "    width, height = 2 * np.sqrt(vals)\n",
    "    angle = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n",
    "    ellipse = Ellipse(xy=mean, width=width, height=height, angle=angle, \n",
    "                edgecolor=colors[i], fc='None', lw=2)\n",
    "    i+=1\n",
    "    ax.add_patch(ellipse)\n",
    "\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_title('Projection of User State Parameters')\n",
    "ax.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function Approx. sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width=64\n",
    "use_jump = False\n",
    "user_state_dict = {\"model_hyp\": {\"layer_width\": [width for _ in range(4)]}}\n",
    "time_dict = {\"model_hyp\": {\"layer_width\": [width for _ in range(3)]}\n",
    "            }\n",
    "jump_dict= {\"model_hyp\": {\"layer_width\": [8, 8]}\n",
    "            }\n",
    "\n",
    "timecheat = False\n",
    "hyperparameter_dict = {\"state_size\": state_size, \"time_model\": time_dict, \n",
    "                           \"state_model\": user_state_dict,\n",
    "                           \"time_embedding_size\" :32, \"max_freq\": 30,}\n",
    "\n",
    "if use_jump:\n",
    "    hyperparameter_dict[\"jump_model\"]= jump_dict\n",
    "train_model = all_in_one_model(hyperparameter_dict, timecheat=timecheat, noise_size=1)\n",
    "print(train_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(dataloader_list)\n",
    "\n",
    "num_epochs = 201\n",
    "warmup_period = steps_per_epoch * 1\n",
    "num_steps = num_epochs*steps_per_epoch - warmup_period\n",
    "num_iter_til_first_restart = (num_steps + 1)//2\n",
    "user_lr = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "optimizer = optim.AdamW(train_model.parameters(), lr=user_lr,\n",
    "                        weight_decay=1e-7)\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=num_iter_til_first_restart, T_mult=1, eta_min=1e-6)\n",
    "\n",
    "warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simtrain.utils import weighted_mse_loss\n",
    "\n",
    "loss_func_kl=partial(utils.kl_divergence, mu2=0, sigma2=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_function_approx_multiple_variational(train_model, path_list, scoring_func=utils.energy_score_loss,\n",
    "            device=device, user_lr=0.01, user_lr_decay=0.995,\n",
    "            loss_func_kl=loss_func_kl, kl_weight=kl_weight, logging_shift=1,\n",
    "            state_size=state_size, warmup_scheduler=warmup_scheduler, lr_scheduler=lr_scheduler,\n",
    "            optimizer=optimizer, num_epochs=num_epochs, num_tries=30, timecheat=timecheat, \n",
    "            loss_print_interval=25, include_jump=use_jump,\n",
    "            warmup_period=warmup_period)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 4\n",
    "sample_path = path_list[user_id]\n",
    "state = torch.tensor([sample_path[1]])\n",
    "len(sample_path[0]), state, f\"id: {sample_path[4]['user_id']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpler nn\n",
    "simulate_single_partial_forced_function_approx = partial(\n",
    "    utils.simulate_single_forced_function_approx, user_data=sample_path,state=state,\n",
    "                              num_tries=100, timecheat=timecheat, state_size=state_size,\n",
    "                              use_jump=use_jump)\n",
    "simulate_single_partial_function_approx = partial(\n",
    "    utils.simulate_single_function_approx, num_events =len(sample_path[0]), state=state,\n",
    "                              num_tries=100, timecheat=timecheat, state_size=state_size,\n",
    "                              jump_data=sample_path[3] if use_jump else False\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intensity\n",
    "example_out_forced = simulate_single_partial_forced_function_approx(train_model)\n",
    "example_out = simulate_single_partial_function_approx(train_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_1 = sample_path[0] # Timestamps for the first time series\n",
    "time_series_2 = torch.clamp(torch.as_tensor(example_out_forced),0,70).detach().numpy()  # Timestamps for the second time series\n",
    "time_series_3 = torch.clamp(torch.as_tensor(example_out), 0, 1000).detach().numpy()  # Timestamps for the second time series\n",
    "\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "ax.scatter(time_series_3, [3] * len(time_series_3), color='green', label='Simmulation Free', s=10, marker='x')\n",
    "\n",
    "ax.scatter(time_series_2, [2] * len(time_series_2), color='red', label='Simmulation Forced', s=10, marker='x')\n",
    "\n",
    "ax.scatter(time_series_1, [1] * len(time_series_1), color='blue', label='Ground Truth', s=10, marker='o')\n",
    "\n",
    "\n",
    "# Add labels, legend, and grid\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_yticks([1, 2, 3])\n",
    "ax.set_yticklabels(['Ground Truth', 'Simulation Forced', \"Simulation Free\"])\n",
    "ax.set_title('Sampled Time series')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), frameon=False)\n",
    "\n",
    "# Adjust subplot parameters to make room for the legend\n",
    "plt.subplots_adjust(right=0.75)\n",
    "ax.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(len(path_list)):\n",
    "    utils.visualize_samples(ind,train_model, path_list, state_size, timecheat=False, \n",
    "                      use_jump=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Models old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter dicts\n",
    "width= 32\n",
    "user_state_dict = {\"model_hyp\": {\"layer_width\": [width, width, width]}}\n",
    "intensity_state_dict = {\"model_hyp\": {\"user_model_hyp\": {\"layer_width\": [width, width],\n",
    "                                        \"noise\": 0},\n",
    "                        \"global_model_hyp\": {\"layer_width\": [width, 3]}}\n",
    "                            }\n",
    "interaction_state_dict = {\"model_hyp\": {\"layer_width\": [width, width ,width]}\n",
    "                            }\n",
    "jump_state_dict = {\"model_hyp\": {\"layer_width\": [width, width]}\n",
    "                        }\n",
    "\n",
    "hyperparameter_dict = {\"state_size\": state_size, \"state_model\": user_state_dict, \n",
    "        \"num_interaction_outcomes\": num_interaction_types,\n",
    "        \"intensity_model\": intensity_state_dict,# \"num_recom\" : num_items_per_recom,\n",
    "        \"recom_dim\":recom_dim, \"interaction_model\": interaction_state_dict,\n",
    "        \"jump_model\": jump_state_dict, \"user_params_size\": state_size,\n",
    "        \"noise\": .0}\n",
    "model = User_simmulation_Model(hyperparameter_dict)\n",
    "#model = Conditioned_User_simmulation_Model(hyperparameter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "path = join(paths.dat, SETTINGS.rootpaths['models'],\n",
    "                 experiment_name, \"user_model.h5\")\n",
    "#model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "warmup_period = len(dataset)\n",
    "num_steps = num_epochs*len(dataset) -warmup_period\n",
    "num_iter_til_first_restart = num_steps//2\n",
    "user_lr = 0.01\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01,\n",
    "                        weight_decay=1e-7)\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=num_iter_til_first_restart, T_mult=1, eta_min=5e-5)\n",
    "\n",
    "warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.print_user_params(dataloader, print_var = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no negative samples\n",
    "#with torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "train(model, dataloader=dataloader, num_epochs=num_epochs, device=device, \n",
    "        loss_func_kl=utils.kl_loss, kl_weight=kl_weight, user_lr=user_lr, \n",
    "        conditioned=conditioned, loss_func= nn.NLLLoss(), \n",
    "        optimizer=optimizer, lr_scheduler=lr_scheduler, num_classes=num_interaction_types, \n",
    "        logger=utils.logging_func, warmup_period=warmup_period, \n",
    "        intensity_loss_func=utils.square_intensity_loss,\n",
    "        state_size=state_size,max_time=max_time, log_step_size=1, \n",
    "        warmup_scheduler = warmup_scheduler,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adsaf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_negatives(model, dataloader=dataloader, num_epochs=num_epochs, device=device, \n",
    "                loss_func=utils.loss_func, loss_func_kl=utils.kl_loss, kl_weight=kl_weight, \n",
    "                user_lr=user_lr, optimizer=optimizer, lr_scheduler=lr_scheduler, \n",
    "                num_classes=num_interaction_types, logger=utils.logging_func,\n",
    "                warmup_period=warmup_period, intensity_loss_func=utils.log_loss,\n",
    "                state_size=state_size,max_time=max_time, log_step_size=1, warmup_scheduler = warmup_scheduler,\n",
    "                num_negatives=num_negatives, positive_examples_weight=5,conditioned=conditioned,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.print_user_params(dataloader, print_var = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "path = join(paths.dat, SETTINGS.rootpaths['models'],\n",
    "                             experiment_name, \"user_model.h5\")\n",
    "\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data(changes during training)\n",
    "path = join(paths.dat, SETTINGS.rootpaths['models'],\n",
    "                             experiment_name, \"data.h5\")\n",
    "torch.save({\n",
    "    'data': dataloader.dataset.data,\n",
    "}, path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditioned square continouous state + state_int new formulas\n",
    "explore_models.plot_rnd_user_visits_new(state_size, max_time=max_time, \n",
    "                dataset=dataloader.dataset, User_model = model, use_true_recommendations =True, \n",
    "                num_classes = num_interaction_types,\n",
    "                        teacher_forcing =True, user_idx=1, conditioned=conditioned, increment=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
