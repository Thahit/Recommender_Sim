{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 15:20:18,263\tINFO worker.py:1558 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf11fcf57b434694afa43af79abc31e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.9.19</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>2.9.0</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.19', ray_version='2.9.0', ray_commit='{{RAY_COMMIT_SHA}}', protocol_version=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from simtrain import SETTINGS_POLIMI as SETTINGS\n",
    "from simtrain.sim_models_new import Toy_intensity_Generator, Toy_intensity_Comparer\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import paths\n",
    "from os.path import join\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import simtrain.utils as utils\n",
    "from tqdm import tqdm\n",
    "import evotorch\n",
    "import ray\n",
    "from functools import partial\n",
    "\n",
    "os.environ['PYTHONPATH'] = \"/home/thahit/github/Recommender_Sim\"\n",
    "ray.init(ignore_reinit_error=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_size = SETTINGS.STATE_SIZE\n",
    "experiment_name = \"toy\"\n",
    "#num_negatives = 100\n",
    "#conditioned=True\n",
    "#kl_weight=.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sfaf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msfaf\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sfaf' is not defined"
     ]
    }
   ],
   "source": [
    "sfaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter dicts\n",
    "width= 8\n",
    "user_state_dict = {\"model_hyp\": {\"layer_width\": [width, width]}}\n",
    "intensity_state_dict = {\"model_hyp\": {\"layer_width\": [width, width],\n",
    "                                                         \"noise\": 0}\n",
    "                            }\n",
    "\n",
    "\n",
    "hyperparameter_dict = {\"state_size\": state_size, \"state_model\": user_state_dict, \n",
    "                           \"intensity_model\": intensity_state_dict,# \"num_recom\" : num_items_per_recom,\n",
    "                            \"noise\": 0.}\n",
    "gen_model = Toy_intensity_Generator(hyperparameter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/thahit/github/Recommender_Sim/dat/saved_models_polimi/simulate_intensity/gnerator_model.h5'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = join(paths.dat, SETTINGS.filepaths_new[\"simulate_intensity_model\"]\n",
    "            )\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.save(gen_model.state_dict(), path)\n",
    "gen_model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.zeros((1, state_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'interval_time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m samples \u001b[38;5;241m=\u001b[39m [gen_model\u001b[38;5;241m.\u001b[39msample_one(state)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)]\n\u001b[1;32m      2\u001b[0m samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(samples)\n\u001b[1;32m      3\u001b[0m samples\n",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m samples \u001b[38;5;241m=\u001b[39m [\u001b[43mgen_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)]\n\u001b[1;32m      2\u001b[0m samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(samples)\n\u001b[1;32m      3\u001b[0m samples\n",
      "File \u001b[0;32m~/github/Recommender_Sim/simtrain/sim_models_new.py:989\u001b[0m, in \u001b[0;36mToy_intensity_Generator.sample_one\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    983\u001b[0m u \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;66;03m# theoretically one should also do -u + 1\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m#h_optimized = self.optimize_h(u)\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m#h_optimized = brentq(self.objective_function, 1e-30, 200, args=(u, state), \u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m#                    maxiter=40, xtol=1e-7,)\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m#h_optimized = self.binary_search(state, u)\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m h_optimized \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_h\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m#print(f\"u: {u} \\t h: {h_optimized}\")\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mflatten(h_optimized)\n",
      "File \u001b[0;32m~/github/Recommender_Sim/simtrain/sim_models_new.py:972\u001b[0m, in \u001b[0;36mToy_intensity_Generator.find_h\u001b[0;34m(self, state, uniform_guess, interval_size, max_iter)\u001b[0m\n\u001b[1;32m    970\u001b[0m curr_state \u001b[38;5;241m=\u001b[39m state\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[0;32m--> 972\u001b[0m     curr_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_state_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    973\u001b[0m     intensity \u001b[38;5;241m=\u001b[39m intensity \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintensity_model(h \u001b[38;5;241m=\u001b[39m interval_size, \n\u001b[1;32m    974\u001b[0m                         intensity\u001b[38;5;241m=\u001b[39mintensity, state\u001b[38;5;241m=\u001b[39mcurr_state)\u001b[38;5;241m*\u001b[39minterval_size)\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m intensity \u001b[38;5;241m>\u001b[39m target:\n",
      "File \u001b[0;32m~/anaconda3/envs/WW2/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/WW2/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'interval_time'"
     ]
    }
   ],
   "source": [
    "samples = [gen_model.sample_one(state).detach().numpy() for _ in range(100)]\n",
    "samples = np.array(samples)\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feed in true values for fake models, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Histogram\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(\u001b[43msamples\u001b[49m, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, density\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m'\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHistogram of Samples\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'samples' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(samples, bins=50, density=True, alpha=0.6, color='g', edgecolor='black')\n",
    "plt.title('Histogram of Samples')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Density Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.kdeplot(samples, shade=True, color='g', bw_adjust=0.1)\n",
    "plt.title('Density Plot of Samples')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'interval_time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample_path \u001b[38;5;241m=\u001b[39m \u001b[43mgen_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m sample_path \u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(sample_path)\n\u001b[1;32m      3\u001b[0m sample_path\n",
      "File \u001b[0;32m~/github/Recommender_Sim/simtrain/sim_models_new.py:1000\u001b[0m, in \u001b[0;36mToy_intensity_Generator.sample_path\u001b[0;34m(self, num_samples, state)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[0;32m-> 1000\u001b[0m         h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m         curr_time \u001b[38;5;241m=\u001b[39m curr_time \u001b[38;5;241m+\u001b[39m h\n\u001b[1;32m   1002\u001b[0m         path\u001b[38;5;241m.\u001b[39mappend(curr_time)\n",
      "File \u001b[0;32m~/github/Recommender_Sim/simtrain/sim_models_new.py:989\u001b[0m, in \u001b[0;36mToy_intensity_Generator.sample_one\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    983\u001b[0m u \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;66;03m# theoretically one should also do -u + 1\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m#h_optimized = self.optimize_h(u)\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m#h_optimized = brentq(self.objective_function, 1e-30, 200, args=(u, state), \u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m#                    maxiter=40, xtol=1e-7,)\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m#h_optimized = self.binary_search(state, u)\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m h_optimized \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_h\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m#print(f\"u: {u} \\t h: {h_optimized}\")\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mflatten(h_optimized)\n",
      "File \u001b[0;32m~/github/Recommender_Sim/simtrain/sim_models_new.py:972\u001b[0m, in \u001b[0;36mToy_intensity_Generator.find_h\u001b[0;34m(self, state, uniform_guess, interval_size, max_iter)\u001b[0m\n\u001b[1;32m    970\u001b[0m curr_state \u001b[38;5;241m=\u001b[39m state\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[0;32m--> 972\u001b[0m     curr_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_state_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    973\u001b[0m     intensity \u001b[38;5;241m=\u001b[39m intensity \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintensity_model(h \u001b[38;5;241m=\u001b[39m interval_size, \n\u001b[1;32m    974\u001b[0m                         intensity\u001b[38;5;241m=\u001b[39mintensity, state\u001b[38;5;241m=\u001b[39mcurr_state)\u001b[38;5;241m*\u001b[39minterval_size)\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m intensity \u001b[38;5;241m>\u001b[39m target:\n",
      "File \u001b[0;32m~/anaconda3/envs/WW2/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/WW2/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'interval_time'"
     ]
    }
   ],
   "source": [
    "sample_path = gen_model.sample_path(num_samples=5)\n",
    "sample_path =torch.tensor(sample_path)\n",
    "sample_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.0833,  6.3958, 11.8125, 28.0417, 28.2708, 40.1042, 42.8542])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load actual data instead\n",
    "checkpoint = torch.load(join(paths.dat, SETTINGS.rootpaths['models'],\n",
    "                             \"testing\", \"data.h5\"))\n",
    "list_of_dicts = checkpoint['data']\n",
    "chosen_sample = list_of_dicts[4][\"timestamps\"]\n",
    "sample_path =torch.tensor(chosen_sample)\n",
    "sample_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.eventplot(sample_path, orientation='vertical', colors='r')\n",
    "plt.scatter(sample_path, [1] * len(sample_path), color='blue', label='Time Series 1', s=10, marker='x')\n",
    "\n",
    "plt.yticks([])\n",
    "plt.xlabel('Time')\n",
    "plt.title('Event Plot of Sparse Binary Events')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdafa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43masdafa\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'asdafa' is not defined"
     ]
    }
   ],
   "source": [
    "asdafa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.0833,  6.3958, 11.8125, 28.0417, 28.2708, 40.1042, 42.8542])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(join(paths.dat, SETTINGS.rootpaths['models'],\n",
    "                             \"testing\", \"data.h5\"))\n",
    "list_of_dicts = checkpoint['data']\n",
    "chosen_sample = list_of_dicts[4][\"timestamps\"]\n",
    "sample_path =torch.tensor(chosen_sample)\n",
    "sample_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.eventplot(sample_path, orientation='vertical', colors='r')\n",
    "plt.scatter(sample_path, [1] * len(sample_path), color='blue', label='Time Series 1', s=10, marker='x')\n",
    "\n",
    "plt.yticks([])\n",
    "plt.xlabel('Time')\n",
    "plt.title('Event Plot of Sparse Binary Events')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Gradient Descent\n",
    "Not working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single(model, path, scoring_func,optimizer, num_epochs=100, num_tries=20):\n",
    "    \n",
    "    for iter in tqdm(range(num_epochs)):\n",
    "        last_t = 0\n",
    "        state = torch.zeros((1, state_size))\n",
    "        loss = 0.\n",
    "        #results = []\n",
    "        for timestep in path:\n",
    "            current_pred = []\n",
    "            for _ in range(num_tries):\n",
    "                current_pred.append(last_t + model.sample_one(state))\n",
    "            current_pred = torch.stack(current_pred)\n",
    "            #results.append(torch.mean(current_pred))\n",
    "            loss += torch.log(scoring_func(current_pred, timestep))\n",
    "            delta = timestep-last_t\n",
    "            last_t = timestep\n",
    "            state = model.evolve_state(state, delta)\n",
    "        print(\"loss: \", loss)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2)\n",
    "        optimizer.step()\n",
    "\n",
    "        #for name, param in model.named_parameters():\n",
    "        #    print(f\"Parameter Name: {name}\")\n",
    "        #    print(f\"Parameter Value: {param}\")\n",
    "        #    print(f\"Gradients: {param.grad}\")\n",
    "        #    print(f\"Parameter Shape: {param.shape}\")\n",
    "        #    print(f\"Requires Gradient: {param.requires_grad}\")\n",
    "        #    print(\"-\" * 40)\n",
    "        #return\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter dicts\n",
    "width= 16\n",
    "user_state_dict = {\"model_hyp\": {\"layer_width\": [width, width, width]}}\n",
    "intensity_state_dict = {\"model_hyp\": {\"layer_width\": [width, width, width],\n",
    "                                                         \"noise\": 0}\n",
    "                            }\n",
    "\n",
    "\n",
    "hyperparameter_dict = {\"state_size\": state_size, \"state_model\": user_state_dict, \n",
    "                           \"intensity_model\": intensity_state_dict,# \"num_recom\" : num_items_per_recom,\n",
    "                            \"noise\": 0.}\n",
    "train_model = Toy_intensity_Generator(hyperparameter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = join(paths.dat, SETTINGS.filepaths_new[\"copy_intensity_model\"]\n",
    "            )\n",
    "#torch.save(gen_model.state_dict(), path_train)\n",
    "#train_model.load_state_dict(torch.load(path_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = torch.as_tensor(sample_path)\n",
    "optimizer = optim.AdamW(train_model.parameters(), lr=0.1,\n",
    "                        weight_decay=1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_single(train_model, sample_path, scoring_func=utils.energy_score_loss,\n",
    "#            optimizer=optimizer, num_epochs=30, num_tries=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function approximation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simtrain.sim_models_new import all_in_one_model\n",
    "\n",
    "width=64\n",
    "user_state_dict = {\"model_hyp\": {\"layer_width\": [width, width, width, width]}}\n",
    "time_dict = {\"model_hyp\": {\"layer_width\": [width, width, width, 3]}\n",
    "                            }\n",
    "\n",
    "timecheat = False\n",
    "hyperparameter_dict = {\"state_size\": state_size, \"time_model\": time_dict, \n",
    "                           \"state_model\": user_state_dict}\n",
    "train_model = all_in_one_model(hyperparameter_dict, timecheat=timecheat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_function_approx(model, path, scoring_func,optimizer, num_epochs=100, \n",
    "                                 num_tries=20, timecheat=False, loss_print_interval=1):\n",
    "    \n",
    "    for iter in tqdm(range(num_epochs)):\n",
    "        last_t = 0\n",
    "        state = torch.zeros((1, state_size))\n",
    "        loss = 0.\n",
    "        mse_loss = nn.MSELoss()\n",
    "        #results = []\n",
    "        for timestep in path:\n",
    "            current_pred = []\n",
    "            for _ in range(num_tries):\n",
    "                if timecheat:\n",
    "                    next_time = model.get_time(state, timestep)\n",
    "                else:\n",
    "                    next_time = model.get_time(state)\n",
    "                #print(f\"next_time: {next_time}, next_state: {next_state}\")\n",
    "                current_pred.append(last_t + next_time)\n",
    "            current_pred = torch.stack(current_pred)\n",
    "            last_t = timestep\n",
    "            #results.append(torch.mean(current_pred))\n",
    "            timestep = torch.Tensor([[timestep]])\n",
    "            #print(torch.mean(current_pred))\n",
    "            #loss = loss + mse_loss(next_time, timestep)\n",
    "            loss += torch.log(scoring_func(current_pred, timestep)+1e-25)# + thing should be useless\n",
    "\n",
    "            state = model.get_new_state(state, timestep)\n",
    "        if iter %loss_print_interval == 0:\n",
    "            print(\"loss: \", loss)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.)\n",
    "        optimizer.step()\n",
    "\n",
    "        #for name, param in model.named_parameters():\n",
    "        #    print(f\"Parameter Name: {name}\")\n",
    "        #    print(f\"Gradients: {param.grad}\")\n",
    "        #    print(f\"Parameter Value: {param}\")\n",
    "        #    print(f\"Parameter Shape: {param.shape}\")\n",
    "        #    print(f\"Requires Gradient: {param.requires_grad}\")\n",
    "        #    print(\"-\" * 40)\n",
    "        #return\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #return results\n",
    "    print(\"loss: \", loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(train_model.parameters(), lr=0.1,\n",
    "                        weight_decay=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_single_function_approx(train_model, sample_path,scoring_func=utils.energy_score_loss,\n",
    "            optimizer=optimizer, num_epochs=100, num_tries=20, timecheat=timecheat, loss_print_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(train_model.parameters(), lr=0.01,\n",
    "                        weight_decay=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_single_function_approx(train_model, sample_path,scoring_func=utils.energy_score_loss,\n",
    "            optimizer=optimizer, num_epochs=200,timecheat=timecheat, num_tries=20, loss_print_interval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(train_model.parameters(), lr=0.001,\n",
    "                        weight_decay=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_single_function_approx(train_model, sample_path, scoring_func=utils.energy_score_loss,\n",
    "            optimizer=optimizer, num_epochs=300,timecheat=timecheat, num_tries=20, loss_print_interval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(train_model.parameters(), lr=0.00001,\n",
    "                        weight_decay=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_single_function_approx(train_model, sample_path,scoring_func=utils.energy_score_loss,\n",
    "            optimizer=optimizer, num_epochs=3000,timecheat=timecheat, num_tries=20, loss_print_interval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adsfasdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sorted = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "width=64\n",
    "user_state_dict = {\"model_hyp\": {\"layer_width\": [width, width, width, width]}}\n",
    "time_dict = {\"model_hyp\": {\"layer_width\": [width, width, width, 3]}\n",
    "            }\n",
    "\n",
    "timecheat = False\n",
    "hyperparameter_dict = {\"state_size\": state_size, \"state_model\": time_dict, \n",
    "                           \"intensity_model\": user_state_dict}\n",
    "model = Toy_intensity_Comparer(hyperparameter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 0.60017, Frequency: 0.0\n",
      "Timestep: 1.60785, Frequency: 0.0\n",
      "Timestep: 2.14246, Frequency: 0.0\n",
      "Timestep: 2.14698, Frequency: 0.0\n",
      "Timestep: 2.38306, Frequency: 0.0\n",
      "Timestep: 3.01250, Frequency: 0.0\n",
      "Timestep: 3.02679, Frequency: 0.0\n",
      "Timestep: 3.11503, Frequency: 0.0\n",
      "Timestep: 4.10295, Frequency: 0.0\n",
      "Timestep: 4.29317, Frequency: 0.0\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TimestepFrequencyDataset(Dataset):\n",
    "    def __init__(self, timesteps, num_random_points=100, interval=0.5, max_time=70):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            timesteps (numpy array): Array of positive timesteps.\n",
    "            num_random_points (int): Number of random points to generate.\n",
    "            interval (float): Interval length to compute frequency.\n",
    "            max_time (float): maximum time the dataset is allowed to contain.\n",
    "        \"\"\"\n",
    "        self.timesteps = np.array(timesteps)\n",
    "        self.num_random_points = num_random_points\n",
    "        self.interval = interval\n",
    "        \n",
    "        # Generate random time points and ensure they include actual timesteps\n",
    "        self.max_time = max_time\n",
    "        self.random_points = np.sort(np.random.uniform(0, self.max_time, self.num_random_points))\n",
    "        self.unique_points = np.unique(np.concatenate([self.random_points, self.timesteps]))\n",
    "        \n",
    "        # Calculate frequencies for each point\n",
    "        self.frequencies = np.array([self.calculate_frequency(point) for point in self.unique_points])\n",
    "        \n",
    "    def calculate_frequency(self, point):\n",
    "        \"\"\"\n",
    "        Calculate frequency of events in the interval [point, point + interval).\n",
    "        \n",
    "        Args:\n",
    "            point (float): The point to calculate frequency for.\n",
    "        \n",
    "        Returns:\n",
    "            float: Frequency of events per interval.\n",
    "        \"\"\"\n",
    "        # could be made more efficient by using the input is sorted\n",
    "        count = np.sum(np.abs(self.timesteps - point) <= self.interval/2)\n",
    "        frequency = count / self.interval\n",
    "        return frequency\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.unique_points)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get item at index `idx`.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the item to fetch.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary with 'timestep' and 'frequency'.\n",
    "        \"\"\"\n",
    "        timestep = self.unique_points[idx]\n",
    "        frequency = self.frequencies[idx]\n",
    "        return {'timestep': torch.tensor(timestep, dtype=torch.float32),\n",
    "                'frequency': torch.tensor(frequency, dtype=torch.float32)}\n",
    "\n",
    "# Example usage\n",
    "dataset = TimestepFrequencyDataset(sample_path)\n",
    "\n",
    "# Accessing items from the dataset\n",
    "for i in range(min(len(dataset), 10)):\n",
    "    sample = dataset[i]\n",
    "    print(f\"Timestep: {sample['timestep'].item():.5f}, Frequency: {sample['frequency'].item()}\")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=not train_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=0.01,\n",
    "                        weight_decay=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_density(model, dataloader, criterion, optimizer, state_size, num_epochs=100, \n",
    "        loss_print_interval=1):\n",
    "    \n",
    "    for iter in tqdm(range(num_epochs)):\n",
    "        loss_sum = .0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            timesteps = batch['timestep'].unsqueeze(1)  # Add batch dimension\n",
    "            frequencies = batch['frequency']\n",
    "            state = torch.zeros((len(timesteps), state_size))\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(state, timesteps)\n",
    "\n",
    "            loss = criterion(outputs, frequencies)  # Remove extra dimension from output\n",
    "            loss_sum += loss.item()\n",
    "            print(f\"loss: {loss} \\tfrequencies: {frequencies} \\t predicted: {outputs}\")\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.)\n",
    "            optimizer.step()    \n",
    "\n",
    "            for name, param in model.named_parameters():\n",
    "                print(f\"Parameter Name: {name}\")\n",
    "                print(f\"Gradients: {param.grad}\")\n",
    "        if iter % loss_print_interval == 0:\n",
    "            print(f\"epch: {iter} loss_sum: {loss_sum :.4f}\")\n",
    "        \n",
    "\n",
    "def train_density_sorted():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/home/thahit/anaconda3/envs/WW2/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/thahit/anaconda3/envs/WW2/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "100%|██████████| 2/2 [00:00<00:00, 22.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2852.1882,  2765.2644,   171.1403,   968.2811],\n",
      "        [-2132.9348,  2065.0857,   126.1791,   724.3677],\n",
      "        [-1629.7666,  1574.5975,    94.2588,   553.8027],\n",
      "        [-2912.3013,  2823.7524,   174.8764,   988.6699],\n",
      "        [-1398.6831,  1349.1907,    79.4961,   475.4850],\n",
      "        [ -117.1047,   144.8941,    29.8103,    36.3426],\n",
      "        [ -514.5488,   505.0840,    35.8804,   173.9211],\n",
      "        [-3832.7124,  3718.8940,   231.8077,  1300.8900],\n",
      "        [-1489.8555,  1438.1182,    85.3170,   506.3852],\n",
      "        [-3635.9739,  3527.5869,   219.6597,  1234.1495],\n",
      "        [ -276.6954,   295.0114,    36.1031,    91.0118],\n",
      "        [  -56.6416,    64.5259,    10.9144,    18.0905],\n",
      "        [ -806.1091,   779.2067,    47.2853,   273.8113],\n",
      "        [ -364.4576,   378.0250,    39.8875,   121.0273],\n",
      "        [-2172.4922,  2103.6145,   128.6663,   737.7803],\n",
      "        [-2599.5205,  2519.3813,   155.4048,   882.5876]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2855264321536.0 \tfrequencies: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) \t predicted: tensor([[2328834.7500],\n",
      "        [1737733.2500],\n",
      "        [1323426.5000],\n",
      "        [2378200.7500],\n",
      "        [1132978.1250],\n",
      "        [ 143908.2656],\n",
      "        [ 426232.5312],\n",
      "        [3133593.7500],\n",
      "        [1208112.2500],\n",
      "        [2972162.5000],\n",
      "        [ 256571.1719],\n",
      "        [  60204.5703],\n",
      "        [ 654178.7500],\n",
      "        [ 322374.6875],\n",
      "        [1770267.5000],\n",
      "        [2121286.7500]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2376.4573,   748.0876,  -912.2697,   977.1989],\n",
      "        [ -375.8820,   117.4868,  -144.2154,   154.5390],\n",
      "        [-1656.5248,   521.1579,  -635.8755,   681.1542],\n",
      "        [-1409.6942,   443.3545,  -541.1132,   579.6546],\n",
      "        [-1658.9631,   521.9265,  -636.8116,   682.1569],\n",
      "        [ -743.2884,   233.2969,  -285.2688,   305.6208],\n",
      "        [-1573.3385,   494.9368,  -603.9390,   646.9471],\n",
      "        [-2723.4990,   857.4785, -1045.5048,  1119.9065],\n",
      "        [ -609.6126,   191.1610,  -233.9484,   250.6517],\n",
      "        [  -73.7539,    22.2531,   -28.2233,    30.3004],\n",
      "        [-1652.1224,   519.7702,  -634.1854,   679.3439],\n",
      "        [-3048.7419,   959.9982, -1170.3711,  1253.6503],\n",
      "        [ -997.8882,   313.5492,  -383.0139,   410.3152],\n",
      "        [ -789.7743,   247.9497,  -303.1155,   324.7363],\n",
      "        [ -596.4790,   187.0211,  -228.9062,   245.2511],\n",
      "        [-1192.1959,   374.7969,  -457.6119,   490.2168]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 300446842880.0 \tfrequencies: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4.]) \t predicted: tensor([[ 828546.8750],\n",
      "        [ 131949.8438],\n",
      "        [ 577889.6875],\n",
      "        [ 491951.5625],\n",
      "        [ 578738.7500],\n",
      "        [ 259915.6875],\n",
      "        [ 548927.1875],\n",
      "        [ 949375.4375],\n",
      "        [ 213360.7344],\n",
      "        [  26868.3281],\n",
      "        [ 576356.9375],\n",
      "        [1062614.3750],\n",
      "        [ 348570.5938],\n",
      "        [ 276103.7500],\n",
      "        [ 208786.3750],\n",
      "        [ 416224.2812]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5494, -0.8182, -0.1110,  0.2074],\n",
      "        [-0.5494, -0.8182, -0.1110,  0.2074],\n",
      "        [-0.5494, -0.8182, -0.1110,  0.2074],\n",
      "        [-0.5494, -0.8182, -0.1110,  0.2074],\n",
      "        [-0.5494, -0.8182, -0.1110,  0.2074],\n",
      "        [-0.5494, -0.8182, -0.1110,  0.2074],\n",
      "        [-0.5494, -0.8182, -0.1110,  0.2074],\n",
      "        [-0.5494, -0.8182, -0.1110,  0.2074],\n",
      "        [-0.5494, -0.8182, -0.1110,  0.2074],\n",
      "        [-0.5494, -0.8182, -0.1110,  0.2074],\n",
      "        [-0.5494, -0.8182, -0.1110,  0.2074],\n",
      "        [-0.5494, -0.8182, -0.1110,  0.2074],\n",
      "        [-0.5494, -0.8182, -0.1110,  0.2074],\n",
      "        [-0.5494, -0.8182, -0.1110,  0.2074],\n",
      "        [-0.5494, -0.8182, -0.1110,  0.2074],\n",
      "        [-0.5494, -0.8182, -0.1110,  0.2074]], grad_fn=<AddmmBackward0>)\n",
      "loss: 51333.96484375 \tfrequencies: tensor([0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 2., 2., 2., 0., 0., 0.]) \t predicted: tensor([[-226.0684],\n",
      "        [-226.0684],\n",
      "        [-226.0684],\n",
      "        [-226.0684],\n",
      "        [-226.0684],\n",
      "        [-226.0684],\n",
      "        [-226.0681],\n",
      "        [-226.0681],\n",
      "        [-226.0681],\n",
      "        [-226.0681],\n",
      "        [-226.0681],\n",
      "        [-226.0681],\n",
      "        [-226.0686],\n",
      "        [-226.0686],\n",
      "        [-226.0686],\n",
      "        [-226.0686]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ -0.5558,  -0.8118,  -0.1174,   0.2010],\n",
      "        [ -0.5558,  -0.8118,  -0.1174,   0.2010],\n",
      "        [ -0.5558,  -0.8118,  -0.1174,   0.2010],\n",
      "        [ -0.5558,  -0.8118,  -0.1174,   0.2010],\n",
      "        [ -0.5558,  -0.8118,  -0.1174,   0.2010],\n",
      "        [ -0.5558,  -0.8118,  -0.1174,   0.2010],\n",
      "        [ -0.5558,  -0.8118,  -0.1174,   0.2010],\n",
      "        [ -0.5558,  -0.8118,  -0.1174,   0.2010],\n",
      "        [-70.5493,  21.4931, -26.2928,  29.7790],\n",
      "        [ -0.5558,  -0.8118,  -0.1174,   0.2010],\n",
      "        [ -0.5558,  -0.8118,  -0.1174,   0.2010],\n",
      "        [ -0.5558,  -0.8118,  -0.1174,   0.2010],\n",
      "        [ -0.5558,  -0.8118,  -0.1174,   0.2010],\n",
      "        [ -0.5558,  -0.8118,  -0.1174,   0.2010],\n",
      "        [ -0.5558,  -0.8118,  -0.1174,   0.2010],\n",
      "        [ -0.5558,  -0.8118,  -0.1174,   0.2010]], grad_fn=<AddmmBackward0>)\n",
      "loss: 81212720.0 \tfrequencies: tensor([0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) \t predicted: tensor([[   186.6854],\n",
      "        [   186.6854],\n",
      "        [   186.6854],\n",
      "        [   186.6854],\n",
      "        [   186.6854],\n",
      "        [   186.6854],\n",
      "        [   186.6854],\n",
      "        [   186.6854],\n",
      "        [-36039.8750],\n",
      "        [   186.6854],\n",
      "        [   186.6854],\n",
      "        [   186.6854],\n",
      "        [   186.6854],\n",
      "        [   186.6854],\n",
      "        [   186.6854],\n",
      "        [   186.6854]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5610, -0.8072, -0.1226,  0.1958],\n",
      "        [-0.5610, -0.8072, -0.1226,  0.1958],\n",
      "        [-0.5610, -0.8072, -0.1226,  0.1958],\n",
      "        [-0.5610, -0.8072, -0.1226,  0.1958],\n",
      "        [-0.5610, -0.8072, -0.1226,  0.1958],\n",
      "        [-0.5610, -0.8072, -0.1226,  0.1958],\n",
      "        [-0.5610, -0.8072, -0.1226,  0.1958],\n",
      "        [-0.5610, -0.8072, -0.1226,  0.1958],\n",
      "        [-0.5610, -0.8072, -0.1226,  0.1958],\n",
      "        [-0.5610, -0.8072, -0.1226,  0.1958],\n",
      "        [-0.5610, -0.8072, -0.1226,  0.1958],\n",
      "        [-0.5610, -0.8072, -0.1226,  0.1958],\n",
      "        [-0.5610, -0.8072, -0.1226,  0.1958],\n",
      "        [-0.5610, -0.8072, -0.1226,  0.1958],\n",
      "        [-0.5610, -0.8072, -0.1226,  0.1958],\n",
      "        [-0.5610, -0.8072, -0.1226,  0.1958]], grad_fn=<AddmmBackward0>)\n",
      "loss: 237309.234375 \tfrequencies: tensor([0., 0., 4., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) \t predicted: tensor([[487.5179],\n",
      "        [487.5179],\n",
      "        [487.5179],\n",
      "        [487.5179],\n",
      "        [487.5179],\n",
      "        [487.5179],\n",
      "        [487.5179],\n",
      "        [487.5179],\n",
      "        [487.5179],\n",
      "        [487.5179],\n",
      "        [487.5179],\n",
      "        [487.5179],\n",
      "        [487.5175],\n",
      "        [487.5175],\n",
      "        [487.5175],\n",
      "        [487.5175]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5598, -0.8011, -0.1232,  0.1960],\n",
      "        [-0.5598, -0.8011, -0.1232,  0.1960],\n",
      "        [-0.5598, -0.8011, -0.1232,  0.1960],\n",
      "        [-0.5598, -0.8011, -0.1232,  0.1960],\n",
      "        [-0.5598, -0.8011, -0.1232,  0.1960],\n",
      "        [-0.5598, -0.8011, -0.1232,  0.1960],\n",
      "        [-0.5598, -0.8011, -0.1232,  0.1960],\n",
      "        [-0.5598, -0.8011, -0.1232,  0.1960],\n",
      "        [-0.5598, -0.8011, -0.1232,  0.1960],\n",
      "        [-0.5598, -0.8011, -0.1232,  0.1960],\n",
      "        [-0.5598, -0.8011, -0.1232,  0.1960],\n",
      "        [-0.5598, -0.8011, -0.1232,  0.1960],\n",
      "        [-0.5598, -0.8011, -0.1232,  0.1960],\n",
      "        [-0.5598, -0.8011, -0.1232,  0.1960],\n",
      "        [-0.5598, -0.8011, -0.1232,  0.1960],\n",
      "        [-0.5598, -0.8011, -0.1232,  0.1960]], grad_fn=<AddmmBackward0>)\n",
      "loss: 138061.015625 \tfrequencies: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0.]) \t predicted: tensor([[371.6904],\n",
      "        [371.6904],\n",
      "        [371.6904],\n",
      "        [371.6904],\n",
      "        [371.6904],\n",
      "        [371.6904],\n",
      "        [371.6902],\n",
      "        [371.6902],\n",
      "        [371.6902],\n",
      "        [371.6902],\n",
      "        [371.6902],\n",
      "        [371.6902],\n",
      "        [371.6903],\n",
      "        [371.6903],\n",
      "        [371.6903],\n",
      "        [371.6903]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5560, -0.7938, -0.1207,  0.1991],\n",
      "        [-0.5560, -0.7938, -0.1207,  0.1991],\n",
      "        [-0.5560, -0.7938, -0.1207,  0.1991],\n",
      "        [-0.5560, -0.7938, -0.1207,  0.1991],\n",
      "        [-0.5560, -0.7938, -0.1207,  0.1991],\n",
      "        [-0.5560, -0.7938, -0.1207,  0.1991],\n",
      "        [-0.5560, -0.7938, -0.1207,  0.1991],\n",
      "        [-0.5560, -0.7938, -0.1207,  0.1991],\n",
      "        [-0.5560, -0.7938, -0.1207,  0.1991],\n",
      "        [-0.5560, -0.7938, -0.1207,  0.1991],\n",
      "        [-0.5560, -0.7938, -0.1207,  0.1991]], grad_fn=<AddmmBackward0>)\n",
      "loss: 8985.033203125 \tfrequencies: tensor([0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0.]) \t predicted: tensor([[94.9695],\n",
      "        [94.9695],\n",
      "        [94.9695],\n",
      "        [94.9695],\n",
      "        [94.9695],\n",
      "        [94.9695],\n",
      "        [94.9695],\n",
      "        [94.9695],\n",
      "        [94.9695],\n",
      "        [94.9695],\n",
      "        [94.9695]], grad_fn=<AddmmBackward0>)\n",
      "epch: 0 loss_sum: 3155792812825.2480\n",
      "tensor([[-0.5509, -0.7856, -0.1162,  0.2039],\n",
      "        [-0.5509, -0.7856, -0.1162,  0.2039],\n",
      "        [-0.5509, -0.7856, -0.1162,  0.2039],\n",
      "        [-0.5509, -0.7856, -0.1162,  0.2039],\n",
      "        [-0.5509, -0.7856, -0.1162,  0.2039],\n",
      "        [-0.5509, -0.7856, -0.1162,  0.2039],\n",
      "        [-0.5509, -0.7856, -0.1162,  0.2039],\n",
      "        [-0.5509, -0.7856, -0.1162,  0.2039],\n",
      "        [-0.5509, -0.7856, -0.1162,  0.2039],\n",
      "        [-0.5509, -0.7856, -0.1162,  0.2039],\n",
      "        [-0.5509, -0.7856, -0.1162,  0.2039],\n",
      "        [-0.5509, -0.7856, -0.1162,  0.2039],\n",
      "        [-0.5509, -0.7856, -0.1162,  0.2039],\n",
      "        [-0.5509, -0.7856, -0.1162,  0.2039],\n",
      "        [-0.5509, -0.7856, -0.1162,  0.2039],\n",
      "        [-0.5509, -0.7856, -0.1162,  0.2039]], grad_fn=<AddmmBackward0>)\n",
      "loss: 76460.96875 \tfrequencies: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) \t predicted: tensor([[-276.5158],\n",
      "        [-276.5158],\n",
      "        [-276.5158],\n",
      "        [-276.5158],\n",
      "        [-276.5158],\n",
      "        [-276.5158],\n",
      "        [-276.5156],\n",
      "        [-276.5156],\n",
      "        [-276.5156],\n",
      "        [-276.5156],\n",
      "        [-276.5156],\n",
      "        [-276.5156],\n",
      "        [-276.5160],\n",
      "        [-276.5160],\n",
      "        [-276.5160],\n",
      "        [-276.5160]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5483, -0.7804, -0.1149,  0.2055],\n",
      "        [-0.5483, -0.7804, -0.1149,  0.2055],\n",
      "        [-0.5483, -0.7804, -0.1149,  0.2055],\n",
      "        [-0.5483, -0.7804, -0.1149,  0.2055],\n",
      "        [-0.5483, -0.7804, -0.1149,  0.2055],\n",
      "        [-0.5483, -0.7804, -0.1149,  0.2055],\n",
      "        [-0.5483, -0.7804, -0.1149,  0.2055],\n",
      "        [-0.5483, -0.7804, -0.1149,  0.2055],\n",
      "        [-0.5483, -0.7804, -0.1149,  0.2055],\n",
      "        [-0.5483, -0.7804, -0.1149,  0.2055],\n",
      "        [-0.5483, -0.7804, -0.1149,  0.2055],\n",
      "        [-0.5483, -0.7804, -0.1149,  0.2055],\n",
      "        [-0.5483, -0.7804, -0.1149,  0.2055],\n",
      "        [-0.5483, -0.7804, -0.1149,  0.2055],\n",
      "        [-0.5483, -0.7804, -0.1149,  0.2055],\n",
      "        [-0.5483, -0.7804, -0.1149,  0.2055]], grad_fn=<AddmmBackward0>)\n",
      "loss: 151604.296875 \tfrequencies: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) \t predicted: tensor([[-389.3675],\n",
      "        [-389.3675],\n",
      "        [-389.3675],\n",
      "        [-389.3675],\n",
      "        [-389.3675],\n",
      "        [-389.3675],\n",
      "        [-389.3673],\n",
      "        [-389.3673],\n",
      "        [-389.3673],\n",
      "        [-389.3673],\n",
      "        [-389.3673],\n",
      "        [-389.3113],\n",
      "        [-389.3676],\n",
      "        [-389.3676],\n",
      "        [-389.3676],\n",
      "        [-389.3676]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5477, -0.7772, -0.1158,  0.2049],\n",
      "        [-0.5477, -0.7772, -0.1158,  0.2049],\n",
      "        [-0.5477, -0.7772, -0.1158,  0.2049],\n",
      "        [-0.5477, -0.7772, -0.1158,  0.2049],\n",
      "        [-0.5477, -0.7772, -0.1158,  0.2049],\n",
      "        [-0.5477, -0.7772, -0.1158,  0.2049],\n",
      "        [-0.5477, -0.7772, -0.1158,  0.2049],\n",
      "        [-0.5477, -0.7772, -0.1158,  0.2049],\n",
      "        [-0.5477, -0.7772, -0.1158,  0.2049],\n",
      "        [-0.5477, -0.7772, -0.1158,  0.2049],\n",
      "        [-0.5477, -0.7772, -0.1158,  0.2049],\n",
      "        [-0.5477, -0.7772, -0.1158,  0.2049],\n",
      "        [-0.5477, -0.7772, -0.1158,  0.2049],\n",
      "        [-0.5477, -0.7772, -0.1158,  0.2049],\n",
      "        [-0.5477, -0.7772, -0.1158,  0.2049],\n",
      "        [-0.5477, -0.7772, -0.1158,  0.2049]], grad_fn=<AddmmBackward0>)\n",
      "loss: 113629.421875 \tfrequencies: tensor([2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) \t predicted: tensor([[-336.9643],\n",
      "        [-336.9643],\n",
      "        [-336.9643],\n",
      "        [-336.9643],\n",
      "        [-336.9643],\n",
      "        [-336.9643],\n",
      "        [-336.9643],\n",
      "        [-336.9643],\n",
      "        [-336.9643],\n",
      "        [-336.9643],\n",
      "        [-336.9643],\n",
      "        [-336.9643],\n",
      "        [-336.9642],\n",
      "        [-336.9642],\n",
      "        [-336.9642],\n",
      "        [-336.9642]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5489, -0.7755, -0.1181,  0.2025],\n",
      "        [-0.5489, -0.7755, -0.1181,  0.2025],\n",
      "        [-0.5489, -0.7755, -0.1181,  0.2025],\n",
      "        [-0.5489, -0.7755, -0.1181,  0.2025],\n",
      "        [-0.5489, -0.7755, -0.1181,  0.2025],\n",
      "        [-0.5489, -0.7755, -0.1181,  0.2025],\n",
      "        [-0.5489, -0.7755, -0.1181,  0.2025],\n",
      "        [-0.5489, -0.7755, -0.1181,  0.2025],\n",
      "        [-0.5489, -0.7755, -0.1181,  0.2025],\n",
      "        [-0.5489, -0.7755, -0.1181,  0.2025],\n",
      "        [-0.5489, -0.7755, -0.1181,  0.2025],\n",
      "        [-0.5489, -0.7755, -0.1181,  0.2025],\n",
      "        [-0.5489, -0.7755, -0.1181,  0.2025],\n",
      "        [-0.5489, -0.7755, -0.1181,  0.2025],\n",
      "        [-0.5489, -0.7755, -0.1181,  0.2025],\n",
      "        [-0.5489, -0.7755, -0.1181,  0.2025]], grad_fn=<AddmmBackward0>)\n",
      "loss: 24870.771484375 \tfrequencies: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 2., 0., 0., 0., 0.]) \t predicted: tensor([[-157.4534],\n",
      "        [-157.4534],\n",
      "        [-157.4534],\n",
      "        [-157.4534],\n",
      "        [-157.4534],\n",
      "        [-157.4534],\n",
      "        [-157.4533],\n",
      "        [-157.4533],\n",
      "        [-157.4533],\n",
      "        [-157.4533],\n",
      "        [-157.4533],\n",
      "        [-157.4533],\n",
      "        [-157.4532],\n",
      "        [-157.4532],\n",
      "        [-157.4532],\n",
      "        [-157.4532]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5517, -0.7754, -0.1213,  0.1989],\n",
      "        [-0.5517, -0.7754, -0.1213,  0.1989],\n",
      "        [-0.5517, -0.7754, -0.1213,  0.1989],\n",
      "        [-0.5517, -0.7754, -0.1213,  0.1989],\n",
      "        [-0.5517, -0.7754, -0.1213,  0.1989],\n",
      "        [-0.5517, -0.7754, -0.1213,  0.1989],\n",
      "        [-0.5517, -0.7754, -0.1213,  0.1989],\n",
      "        [-0.5517, -0.7754, -0.1213,  0.1989],\n",
      "        [-0.5517, -0.7754, -0.1213,  0.1989],\n",
      "        [-0.5517, -0.7754, -0.1213,  0.1989],\n",
      "        [-0.5517, -0.7754, -0.1213,  0.1989],\n",
      "        [-0.5517, -0.7754, -0.1213,  0.1989],\n",
      "        [-0.5517, -0.7754, -0.1213,  0.1989],\n",
      "        [-0.5517, -0.7754, -0.1213,  0.1989],\n",
      "        [-0.5517, -0.7754, -0.1213,  0.1989],\n",
      "        [-0.5517, -0.7754, -0.1213,  0.1989]], grad_fn=<AddmmBackward0>)\n",
      "loss: 5861.4599609375 \tfrequencies: tensor([0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 2., 0., 0., 2., 0.]) \t predicted: tensor([[76.9313],\n",
      "        [76.9313],\n",
      "        [76.9313],\n",
      "        [76.9313],\n",
      "        [76.9313],\n",
      "        [76.9313],\n",
      "        [76.9311],\n",
      "        [76.9311],\n",
      "        [76.9311],\n",
      "        [76.9311],\n",
      "        [76.9311],\n",
      "        [76.9311],\n",
      "        [76.9312],\n",
      "        [76.9312],\n",
      "        [76.9312],\n",
      "        [76.9312]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5522, -0.7735, -0.1238,  0.1977],\n",
      "        [-0.5522, -0.7735, -0.1238,  0.1977],\n",
      "        [-0.5522, -0.7735, -0.1238,  0.1977],\n",
      "        [-0.5522, -0.7735, -0.1238,  0.1977],\n",
      "        [-0.5522, -0.7735, -0.1238,  0.1977],\n",
      "        [-0.5522, -0.7735, -0.1238,  0.1977],\n",
      "        [-0.5522, -0.7735, -0.1238,  0.1977],\n",
      "        [-0.5522, -0.7735, -0.1238,  0.1977],\n",
      "        [-0.5522, -0.7735, -0.1238,  0.1977],\n",
      "        [-0.5522, -0.7735, -0.1238,  0.1977],\n",
      "        [-0.5522, -0.7735, -0.1238,  0.1977],\n",
      "        [-0.5522, -0.7735, -0.1238,  0.1977],\n",
      "        [-0.5522, -0.7735, -0.1238,  0.1977],\n",
      "        [-0.5522, -0.7735, -0.1238,  0.1977],\n",
      "        [-0.5522, -0.7735, -0.1238,  0.1977],\n",
      "        [-0.5522, -0.7735, -0.1238,  0.1977]], grad_fn=<AddmmBackward0>)\n",
      "loss: 21146.494140625 \tfrequencies: tensor([0., 0., 0., 0., 4., 2., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0.]) \t predicted: tensor([[145.9140],\n",
      "        [145.9140],\n",
      "        [145.9140],\n",
      "        [145.9140],\n",
      "        [145.9140],\n",
      "        [145.9140],\n",
      "        [145.9142],\n",
      "        [145.9142],\n",
      "        [145.9142],\n",
      "        [145.9142],\n",
      "        [145.9142],\n",
      "        [145.9142],\n",
      "        [145.9139],\n",
      "        [145.9139],\n",
      "        [145.9139],\n",
      "        [145.9139]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5509, -0.7702, -0.1254,  0.1984],\n",
      "        [-0.5509, -0.7702, -0.1254,  0.1984],\n",
      "        [-0.5509, -0.7702, -0.1254,  0.1984],\n",
      "        [-0.5509, -0.7702, -0.1254,  0.1984],\n",
      "        [-0.5509, -0.7702, -0.1254,  0.1984],\n",
      "        [-0.5509, -0.7702, -0.1254,  0.1984],\n",
      "        [-0.5509, -0.7702, -0.1254,  0.1984],\n",
      "        [-0.5509, -0.7702, -0.1254,  0.1984],\n",
      "        [-0.5509, -0.7702, -0.1254,  0.1984],\n",
      "        [-0.5509, -0.7702, -0.1254,  0.1984],\n",
      "        [-0.5509, -0.7702, -0.1254,  0.1984]], grad_fn=<AddmmBackward0>)\n",
      "loss: 8831.271484375 \tfrequencies: tensor([0., 0., 0., 0., 4., 0., 0., 0., 0., 0., 0.]) \t predicted: tensor([[94.3314],\n",
      "        [94.3314],\n",
      "        [94.3314],\n",
      "        [94.3314],\n",
      "        [94.3314],\n",
      "        [94.3314],\n",
      "        [94.3314],\n",
      "        [94.3314],\n",
      "        [94.3315],\n",
      "        [94.3315],\n",
      "        [94.3315]], grad_fn=<AddmmBackward0>)\n",
      "epch: 1 loss_sum: 402404.6846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_density(model, dataloader, criterion=nn.MSELoss(), state_size=state_size,\n",
    "               optimizer=optimizer, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train via Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evotorch.algorithms import PGPE, CMAES\n",
    "from evotorch.logging import PandasLogger\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter dicts\n",
    "width= 64\n",
    "user_state_dict = {\"model_hyp\": {\"layer_width\": [width,width, width]}}\n",
    "intensity_state_dict = {\"model_hyp\": {\"layer_width\": [width, width,width],\n",
    "                                                         \"noise\": 0}\n",
    "                            }\n",
    "\n",
    "\n",
    "hyperparameter_dict = {\"state_size\": state_size, \"state_model\": user_state_dict, \n",
    "                           \"intensity_model\": intensity_state_dict,# \"num_recom\" : num_items_per_recom,\n",
    "                            \"noise\": 0.}\n",
    "train_model = Toy_intensity_Generator(hyperparameter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = join(paths.dat, SETTINGS.filepaths_new[\"copy_intensity_model\"]\n",
    "            )\n",
    "#torch.save(gen_model.state_dict(), path_train)\n",
    "#train_model.load_state_dict(torch.load(path_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_single(model, path, scoring_func, num_tries=20):\n",
    "    \n",
    "    last_t = 0\n",
    "    state = torch.zeros((1, state_size))\n",
    "    loss = 0.\n",
    "    #results = []\n",
    "    with torch.no_grad():\n",
    "        for timestep in path:\n",
    "            current_pred = []\n",
    "            for _ in range(num_tries):\n",
    "                out = model.sample_one(state)\n",
    "                current_pred.append(last_t + out)\n",
    "            current_pred = torch.stack(current_pred)\n",
    "            #results.append(torch.mean(current_pred))\n",
    "            loss += scoring_func(current_pred, timestep)\n",
    "            delta = timestep-last_t\n",
    "            last_t = timestep\n",
    "            state = model.evolve_state(state, delta)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "eval_single_partial = partial(eval_single, path=sample_path, scoring_func=utils.energy_score_loss,\n",
    "                              num_tries=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_single_partial(train_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Single_path_problem = evotorch.neuroevolution.NEProblem(\n",
    "    objective_sense=\"min\",\n",
    "    network= train_model,\n",
    "    #network=Toy_intensity_Generator,\n",
    "    #network_args = {\"hyperparameter_dict\": hyperparameter_dict},\n",
    "    network_eval_func=eval_single_partial,\n",
    "    device=device,\n",
    "    num_actors = 20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "searcher = PGPE(\n",
    "    Single_path_problem,\n",
    "    popsize=40,\n",
    "    radius_init=5.,\n",
    "    center_learning_rate=0.2,\n",
    "    stdev_learning_rate=0.05,\n",
    ")\n",
    "'''\n",
    "searcher = CMAES(\n",
    "    Single_path_problem,\n",
    "    popsize=20,  \n",
    "    stdev_init= 2.\n",
    ")'''\n",
    "\n",
    "logger = PandasLogger(searcher)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20\n",
    "for _ in tqdm(range(num_iterations), desc=\"Running PGPE\"):\n",
    "    searcher.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 epoch = 50(population) * 20(samples for expectation) * 1(number of paths) * 10'000(integration/for loop steps, worst case) * (2+1) (neural network calls) *10(number of events)= 300m NN calls per epoch\n",
    "\n",
    "for much worse approximation and still way too much compute:<br>\n",
    "1 epoch = 20(population) * 20(samples for expectation) * 1(number of paths) * 100(integration/for loop steps, worst case) * (2+1) (neural network calls) *10(number of events)= 1.2m NN calls per epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm/rnn  feed last hidden state + actual state to predict probability increase,  stop at  threshhold chosen by uniform \n",
    "\n",
    "\n",
    "neural network(time? + state)  -> time + state\n",
    "\n",
    "noise for time but not state\n",
    "\n",
    "normalizing flows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.to_dataframe().mean_eval.plot()# test datapoint CMA ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdcas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_network = Single_path_problem.parameterize_net(searcher.status[\"center\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = join(paths.dat, SETTINGS.filepaths_new[\"copy_intensity_model\"]\n",
    "            )\n",
    "#torch.save(trained_network.state_dict(), path_train)\n",
    "#train_model.load_state_dict(torch.load(path_train))\n",
    "#trained_network = train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_network = train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_single_forced(model, path, num_tries=20):\n",
    "    \n",
    "    last_t = 0\n",
    "    state = torch.zeros((1, state_size))\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for timestep in path:\n",
    "            current_pred = []\n",
    "            for _ in range(num_tries):\n",
    "                out = model.sample_one(state)\n",
    "                current_pred.append(last_t + out)\n",
    "            current_pred = torch.stack(current_pred)\n",
    "            results.append(torch.mean(current_pred))\n",
    "            print(f\"var(forced): {torch.var(current_pred)}\")\n",
    "            #loss += scoring_func(current_pred, timestep)\n",
    "            delta = timestep-last_t\n",
    "            last_t = timestep\n",
    "            state = model.evolve_state(state, delta)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def simulate_single_forced_function_approx(model, path, \n",
    "                                           timecheat=False, num_tries=20):\n",
    "    \n",
    "    last_t = 0\n",
    "    state = torch.zeros((1, state_size))\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for timestep in path:\n",
    "            current_pred = []\n",
    "            for _ in range(num_tries):\n",
    "                if timecheat:\n",
    "                    next_time=model.get_time(state, timestep)\n",
    "                else:\n",
    "                    next_time=model.get_time(state)\n",
    "                #print(f\"next_time: {next_time}, next_state: {next_state}\")\n",
    "                current_pred.append(last_t + next_time[0])\n",
    "            current_pred = torch.stack(current_pred)\n",
    "            results.append(torch.mean(current_pred))\n",
    "            last_t = timestep\n",
    "            state = model.get_new_state(state, torch.tensor([[timestep]]))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def simulate_single_function_approx(model, path, timecheat=False, num_tries=20):\n",
    "    \n",
    "    last_t = 0\n",
    "    state = torch.zeros((1, state_size))\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for timestep in path:\n",
    "            current_pred = []\n",
    "            for _ in range(num_tries):\n",
    "                if timecheat:\n",
    "                    next_time=model.get_time(state, timestep)\n",
    "                else:\n",
    "                    next_time=model.get_time(state)\n",
    "                \n",
    "                current_pred.append(last_t + next_time[0])\n",
    "            current_pred = torch.stack(current_pred)\n",
    "            selected = torch.mean(current_pred)\n",
    "            results.append(selected)\n",
    "            last_t = selected\n",
    "            state = model.get_new_state(state, torch.tensor([[selected]]))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def simulate_single(model, num_events, num_tries=20):\n",
    "    \n",
    "    last_t = 0\n",
    "    state = torch.zeros((1, state_size))\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_events):\n",
    "            current_pred = []\n",
    "            for _ in range(num_tries):\n",
    "                out = model.sample_one(state)\n",
    "                current_pred.append(last_t + out)\n",
    "            current_pred = torch.stack(current_pred)\n",
    "            print(f\"var: {torch.var(current_pred)}\")\n",
    "            selected = torch.mean(current_pred)\n",
    "            results.append(selected)\n",
    "            delta = selected-last_t\n",
    "            last_t = selected\n",
    "            state = model.evolve_state(state, delta)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpler nn\n",
    "simulate_single_partial_forced_function_approx = partial(\n",
    "    simulate_single_forced_function_approx, path=sample_path,\n",
    "                              num_tries=30, timecheat=timecheat)\n",
    "simulate_single_partial_function_approx =partial(\n",
    "    simulate_single_function_approx, path=sample_path,\n",
    "                              num_tries=30, timecheat=timecheat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrals\n",
    "simulate_single_partial_forced = partial(simulate_single_forced, path=sample_path, \n",
    "                              num_tries=2)\n",
    "\n",
    "simulate_single_partial = partial(simulate_single, num_events=12,\n",
    "                              num_tries=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_out_forced = simulate_single_partial_forced(trained_network)\n",
    "example_out = simulate_single_partial(trained_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp\n",
    "example_out_forced = simulate_single_partial_forced_function_approx(trained_network)\n",
    "example_out = simulate_single_partial_function_approx(trained_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data: replace these with your actual time series data\n",
    "time_series_1 = sample_path # Timestamps for the first time series\n",
    "time_series_2 = torch.clamp(torch.as_tensor(example_out_forced),0,70).detach().numpy()  # Timestamps for the second time series\n",
    "time_series_3 = torch.clamp(torch.as_tensor(example_out), 0, 70).detach().numpy()  # Timestamps for the second time series\n",
    "\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Plot the first time series\n",
    "ax.scatter(time_series_1, [1] * len(time_series_1), color='blue', label='Ground Truth', s=10, marker='o')\n",
    "\n",
    "# Plot the second time series\n",
    "ax.scatter(time_series_2, [2] * len(time_series_2), color='red', label='Simmulation Forced', s=10, marker='x')\n",
    "\n",
    "ax.scatter(time_series_3, [3] * len(time_series_3), color='green', label='Simmulation Free', s=10, marker='x')\n",
    "\n",
    "\n",
    "# Add labels, legend, and grid\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_yticks([1, 2, 3])\n",
    "ax.set_yticklabels(['Ground Truth', 'Simmulation Forced', \"Simmulation Free\"])\n",
    "ax.set_title('Comparison of Two Time Series')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), frameon=False)\n",
    "\n",
    "# Adjust subplot parameters to make room for the legend\n",
    "plt.subplots_adjust(right=0.75)\n",
    "ax.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data: replace these with your actual time series data\n",
    "time_series_1 = sample_path # Timestamps for the first time series\n",
    "time_series_2 = torch.clamp(torch.as_tensor(example_out_forced),0,70).detach().numpy()  # Timestamps for the second time series\n",
    "time_series_3 = torch.clamp(torch.as_tensor(example_out), 0, 70).detach().numpy()  # Timestamps for the second time series\n",
    "\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Plot the first time series\n",
    "ax.scatter(time_series_1, [1] * len(time_series_1), color='blue', label='Ground Truth', s=10, marker='o')\n",
    "\n",
    "# Plot the second time series\n",
    "ax.scatter(time_series_2, [2] * len(time_series_2), color='red', label='Simmulation Forced', s=10, marker='x')\n",
    "\n",
    "ax.scatter(time_series_3, [3] * len(time_series_3), color='green', label='Simmulation Free', s=10, marker='x')\n",
    "\n",
    "\n",
    "# Add labels, legend, and grid\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_yticks([1, 2, 3])\n",
    "ax.set_yticklabels(['Ground Truth', 'Simmulation Forced', \"Simmulation Free\"])\n",
    "ax.set_title('Comparison of Two Time Series')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), frameon=False)\n",
    "\n",
    "# Adjust subplot parameters to make room for the legend\n",
    "plt.subplots_adjust(right=0.75)\n",
    "ax.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data: replace these with your actual time series data\n",
    "time_series_1 = sample_path # Timestamps for the first time series\n",
    "time_series_2 = torch.clamp(torch.as_tensor(example_out_forced),0,70).detach().numpy()  # Timestamps for the second time series\n",
    "time_series_3 = torch.clamp(torch.as_tensor(example_out), 0, 70).detach().numpy()  # Timestamps for the second time series\n",
    "\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Plot the first time series\n",
    "ax.scatter(time_series_1, [1] * len(time_series_1), color='blue', label='Ground Truth', s=10, marker='o')\n",
    "\n",
    "# Plot the second time series\n",
    "ax.scatter(time_series_2, [2] * len(time_series_2), color='red', label='Simmulation Forced', s=10, marker='x')\n",
    "\n",
    "ax.scatter(time_series_3, [3] * len(time_series_3), color='green', label='Simmulation Free', s=10, marker='x')\n",
    "\n",
    "\n",
    "# Add labels, legend, and grid\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_yticks([1, 2, 3])\n",
    "ax.set_yticklabels(['Ground Truth', 'Simmulation Forced', \"Simmulation Free\"])\n",
    "ax.set_title('Comparison of Two Time Series')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), frameon=False)\n",
    "\n",
    "# Adjust subplot parameters to make room for the legend\n",
    "plt.subplots_adjust(right=0.75)\n",
    "ax.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scsdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.zeros((1, state_size))\n",
    "samples = [train_model.sample_one(state).detach().numpy() for _ in range(100)]\n",
    "samples = np.array(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(samples, bins=50, density=True, alpha=0.6, color='g', edgecolor='black')\n",
    "plt.title('Histogram of Samples')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Density Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.kdeplot(samples, shade=True, color='g', bw_adjust=0.1)\n",
    "plt.title('Density Plot of Samples')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple paths\n",
    "\n",
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_paths = [torch.as_tensor(gen_model.sample_path(num_samples=7 + i%10)) for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(model, paths, scoring_func, optimizer, num_epochs=100, num_tries=20):\n",
    "    \n",
    "    for iter in tqdm(range(num_epochs)):\n",
    "        avg_loss = 0\n",
    "        #results = []\n",
    "        for path in paths:\n",
    "            last_t = 0\n",
    "            state = torch.zeros((1, state_size))\n",
    "            loss = 0.\n",
    "            for timestep in path:\n",
    "                current_pred = []\n",
    "                for _ in range(num_tries):\n",
    "                    current_pred.append(last_t + model.sample_one(state))\n",
    "                current_pred = torch.stack(current_pred)\n",
    "                #results.append(torch.mean(current_pred))\n",
    "                loss += scoring_func(current_pred, timestep)\n",
    "                delta = timestep-last_t\n",
    "                last_t = timestep\n",
    "                state = model.evolve_state(state, delta)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            avg_loss += loss\n",
    "        print(\"loss: \", loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(train_model, sample_paths, scoring_func=utils.energy_score_loss,\n",
    "                optimizer=optimizer, num_epochs=30, num_tries=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
